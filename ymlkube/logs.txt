
==> Audit <==
|---------|--------------------------------|----------|------|---------|---------------------|---------------------|
| Command |              Args              | Profile  | User | Version |     Start Time      |      End Time       |
|---------|--------------------------------|----------|------|---------|---------------------|---------------------|
| start   | --driver=virtualbox            | minikube | joji | v1.33.1 | 21 Jul 24 12:47 WIB |                     |
|         | --kubernetes-version v1.15.12  |          |      |         |                     |                     |
| start   | --driver=virtualbox            | minikube | joji | v1.33.1 | 21 Jul 24 12:47 WIB |                     |
|         | --kubernetes-version v1.15.12  |          |      |         |                     |                     |
|         | --force                        |          |      |         |                     |                     |
| config  | set driver virtualbox          | minikube | joji | v1.33.1 | 21 Jul 24 12:47 WIB | 21 Jul 24 12:47 WIB |
| config  | set driver virtualbox          | minikube | joji | v1.33.1 | 21 Jul 24 12:47 WIB | 21 Jul 24 12:47 WIB |
| start   | --kubernetes-version v1.15.12  | minikube | joji | v1.33.1 | 21 Jul 24 12:47 WIB |                     |
| start   |                                | minikube | joji | v1.33.1 | 21 Jul 24 12:47 WIB |                     |
| start   |                                | minikube | joji | v1.33.1 | 21 Jul 24 12:49 WIB |                     |
| start   |                                | minikube | joji | v1.33.1 | 21 Jul 24 12:50 WIB |                     |
| start   |                                | minikube | joji | v1.33.1 | 21 Jul 24 12:50 WIB |                     |
| start   |                                | minikube | joji | v1.33.1 | 21 Jul 24 12:50 WIB |                     |
| start   |                                | minikube | joji | v1.33.1 | 21 Jul 24 12:51 WIB |                     |
| delete  |                                | minikube | joji | v1.33.1 | 21 Jul 24 12:52 WIB | 21 Jul 24 12:52 WIB |
| delete  |                                | minikube | joji | v1.33.1 | 21 Jul 24 12:52 WIB | 21 Jul 24 12:52 WIB |
| start   |                                | minikube | joji | v1.33.1 | 21 Jul 24 12:52 WIB |                     |
| start   |                                | minikube | joji | v1.33.1 | 21 Jul 24 12:53 WIB |                     |
| start   | --driver=virtualbox            | minikube | joji | v1.33.1 | 21 Jul 24 12:54 WIB |                     |
|         | --host-only-cidr               |          |      |         |                     |                     |
|         | 192.168.59.1/24                |          |      |         |                     |                     |
| start   |                                | minikube | joji | v1.33.1 | 21 Jul 24 12:56 WIB |                     |
| start   |                                | minikube | joji | v1.33.1 | 21 Jul 24 12:57 WIB |                     |
| start   |                                | minikube | joji | v1.33.1 | 21 Jul 24 12:59 WIB |                     |
| delete  |                                | minikube | joji | v1.33.1 | 21 Jul 24 13:00 WIB | 21 Jul 24 13:00 WIB |
| start   | --driver=virtualbox            | minikube | joji | v1.33.1 | 21 Jul 24 13:00 WIB |                     |
| delete  | --all                          | minikube | joji | v1.33.1 | 21 Jul 24 13:01 WIB | 21 Jul 24 13:01 WIB |
| start   | --driver=virtualbox            | minikube | joji | v1.33.1 | 21 Jul 24 13:01 WIB |                     |
| start   | --driver=docker                | minikube | joji | v1.33.1 | 21 Jul 24 13:02 WIB |                     |
| delete  |                                | minikube | joji | v1.33.1 | 21 Jul 24 13:02 WIB | 21 Jul 24 13:02 WIB |
| start   | --driver=docker                | minikube | joji | v1.33.1 | 21 Jul 24 13:02 WIB | 21 Jul 24 13:03 WIB |
| stop    |                                | minikube | joji | v1.33.1 | 21 Jul 24 13:03 WIB | 21 Jul 24 13:03 WIB |
| stop    |                                | minikube | joji | v1.33.1 | 21 Jul 24 13:03 WIB | 21 Jul 24 13:03 WIB |
| delete  | --all                          | minikube | joji | v1.33.1 | 21 Jul 24 13:04 WIB | 21 Jul 24 13:04 WIB |
| start   | --driver=virtualbox            | minikube | joji | v1.33.1 | 21 Jul 24 13:04 WIB |                     |
| delete  | --all                          | minikube | joji | v1.33.1 | 21 Jul 24 13:05 WIB | 21 Jul 24 13:05 WIB |
| start   |                                | minikube | joji | v1.33.1 | 21 Jul 24 16:45 WIB |                     |
| start   | --driver=docker                | minikube | joji | v1.33.1 | 21 Jul 24 16:45 WIB |                     |
| delete  | --all                          | minikube | joji | v1.33.1 | 21 Jul 24 16:45 WIB | 21 Jul 24 16:45 WIB |
| config  | set driver docker              | minikube | joji | v1.33.1 | 21 Jul 24 16:45 WIB | 21 Jul 24 16:45 WIB |
| start   |                                | minikube | joji | v1.33.1 | 21 Jul 24 16:46 WIB | 21 Jul 24 16:46 WIB |
| service |                                | minikube | joji | v1.33.1 | 21 Jul 24 19:11 WIB |                     |
| service | service-nginx                  | minikube | joji | v1.33.1 | 21 Jul 24 19:19 WIB |                     |
| service | service-nginx                  | minikube | joji | v1.33.1 | 21 Jul 24 19:19 WIB |                     |
| service | service-nginx                  | minikube | joji | v1.33.1 | 21 Jul 24 19:21 WIB |                     |
| service | service-nginx                  | minikube | joji | v1.33.1 | 21 Jul 24 19:35 WIB |                     |
| service | service-nginx                  | minikube | joji | v1.33.1 | 21 Jul 24 19:39 WIB |                     |
| service | service-nginx                  | minikube | joji | v1.33.1 | 21 Jul 24 19:40 WIB |                     |
| service | service-nginx                  | minikube | joji | v1.33.1 | 21 Jul 24 19:40 WIB |                     |
| service | service-nginx                  | minikube | joji | v1.33.1 | 21 Jul 24 19:40 WIB |                     |
| service | service-nginx                  | minikube | joji | v1.33.1 | 21 Jul 24 19:41 WIB |                     |
| service | service-nginx                  | minikube | joji | v1.33.1 | 21 Jul 24 19:41 WIB |                     |
| service | service-nginx                  | minikube | joji | v1.33.1 | 21 Jul 24 19:42 WIB |                     |
| service | service-nginx                  | minikube | joji | v1.33.1 | 21 Jul 24 19:42 WIB |                     |
| service | service-nginx                  | minikube | joji | v1.33.1 | 21 Jul 24 19:42 WIB |                     |
| service | service-nginx                  | minikube | joji | v1.33.1 | 21 Jul 24 19:43 WIB | 21 Jul 24 19:43 WIB |
| service | service-nginx                  | minikube | joji | v1.33.1 | 21 Jul 24 19:44 WIB |                     |
| service | service-nginx                  | minikube | joji | v1.33.1 | 21 Jul 24 19:44 WIB | 21 Jul 24 19:44 WIB |
| service | service-nginx                  | minikube | joji | v1.33.1 | 21 Jul 24 19:45 WIB | 21 Jul 24 19:49 WIB |
| service | service-nginx                  | minikube | joji | v1.33.1 | 21 Jul 24 19:49 WIB |                     |
| start   |                                | minikube | joji | v1.33.1 | 21 Jul 24 20:40 WIB | 21 Jul 24 20:40 WIB |
| service | service-nginx-loadbalancer     | minikube | joji | v1.33.1 | 21 Jul 24 20:44 WIB |                     |
| service | service-nginx-loadbalancer     | minikube | joji | v1.33.1 | 21 Jul 24 20:45 WIB |                     |
| service | service-nginx-loadbalancer     | minikube | joji | v1.33.1 | 21 Jul 24 20:46 WIB |                     |
| service | service-nginx-loadbalancer     | minikube | joji | v1.33.1 | 21 Jul 24 20:47 WIB |                     |
|---------|--------------------------------|----------|------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2024/07/21 20:40:33
Running on machine: joji
Binary: Built with gc go1.22.1 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0721 20:40:33.309537    2198 out.go:291] Setting OutFile to fd 1 ...
I0721 20:40:33.309663    2198 out.go:343] isatty.IsTerminal(1) = true
I0721 20:40:33.309666    2198 out.go:304] Setting ErrFile to fd 2...
I0721 20:40:33.309670    2198 out.go:343] isatty.IsTerminal(2) = true
I0721 20:40:33.309804    2198 root.go:338] Updating PATH: /home/joji/.minikube/bin
I0721 20:40:33.312600    2198 out.go:298] Setting JSON to false
I0721 20:40:33.314084    2198 start.go:129] hostinfo: {"hostname":"joji","uptime":46,"bootTime":1721569187,"procs":70,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"5.15.153.1-microsoft-standard-WSL2","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"guest","hostId":"54bce3ff-141b-419c-96c4-6cd0bef53482"}
I0721 20:40:33.314138    2198 start.go:139] virtualization:  guest
I0721 20:40:33.316576    2198 out.go:177] üòÑ  minikube v1.33.1 on Ubuntu 22.04 (amd64)
I0721 20:40:33.326968    2198 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0721 20:40:33.327133    2198 notify.go:220] Checking for updates...
I0721 20:40:33.330185    2198 driver.go:392] Setting default libvirt URI to qemu:///system
I0721 20:40:33.360562    2198 docker.go:122] docker version: linux-26.1.4:Docker Desktop
I0721 20:40:33.360674    2198 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0721 20:40:33.531524    2198 info.go:266] docker info: {ID:a4446cf4-ce9b-4cd7-bd96-f15ec45558bf Containers:38 ContainersRunning:37 ContainersPaused:0 ContainersStopped:1 Images:13 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:155 OomKillDisable:true NGoroutines:151 SystemTime:2024-07-21 13:40:33.517827241 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:10 KernelVersion:5.15.153.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:6 MemTotal:8303099904 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///var/run/docker-cli.sock] ExperimentalBuild:false ServerVersion:26.1.4 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:d2d58213f83a351ca8f528a95fbd145f5654e957 Expected:d2d58213f83a351ca8f528a95fbd145f5654e957} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.14.1-desktop.1] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.27.1-desktop.1] map[Name:debug Path:/usr/local/lib/docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.32] map[Name:dev Path:/usr/local/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.24] map[Name:feedback Path:/usr/local/lib/docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:/usr/local/lib/docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.2.0] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/usr/local/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.9.3]] Warnings:<nil>}}
I0721 20:40:33.531603    2198 docker.go:295] overlay module found
I0721 20:40:33.534507    2198 out.go:177] ‚ú®  Using the docker driver based on existing profile
I0721 20:40:33.537353    2198 start.go:297] selected driver: docker
I0721 20:40:33.537364    2198 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/joji:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0721 20:40:33.537430    2198 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0721 20:40:33.537501    2198 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0721 20:40:33.682085    2198 info.go:266] docker info: {ID:a4446cf4-ce9b-4cd7-bd96-f15ec45558bf Containers:38 ContainersRunning:37 ContainersPaused:0 ContainersStopped:1 Images:13 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:155 OomKillDisable:true NGoroutines:151 SystemTime:2024-07-21 13:40:33.672509576 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:10 KernelVersion:5.15.153.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:6 MemTotal:8303099904 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///var/run/docker-cli.sock] ExperimentalBuild:false ServerVersion:26.1.4 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:d2d58213f83a351ca8f528a95fbd145f5654e957 Expected:d2d58213f83a351ca8f528a95fbd145f5654e957} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.14.1-desktop.1] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.27.1-desktop.1] map[Name:debug Path:/usr/local/lib/docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.32] map[Name:dev Path:/usr/local/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.24] map[Name:feedback Path:/usr/local/lib/docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:/usr/local/lib/docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.2.0] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/usr/local/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.9.3]] Warnings:<nil>}}
I0721 20:40:33.682523    2198 cni.go:84] Creating CNI manager for ""
I0721 20:40:33.682532    2198 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0721 20:40:33.682561    2198 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/joji:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0721 20:40:33.685356    2198 out.go:177] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I0721 20:40:33.687089    2198 cache.go:121] Beginning downloading kic base image for docker with docker
I0721 20:40:33.688873    2198 out.go:177] üöú  Pulling base image v0.0.44 ...
I0721 20:40:33.691045    2198 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0721 20:40:33.691083    2198 preload.go:147] Found local preload: /home/joji/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4
I0721 20:40:33.691088    2198 cache.go:56] Caching tarball of preloaded images
I0721 20:40:33.691145    2198 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e in local docker daemon
I0721 20:40:33.691201    2198 preload.go:173] Found /home/joji/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0721 20:40:33.691208    2198 cache.go:59] Finished verifying existence of preloaded tar for v1.30.0 on docker
I0721 20:40:33.691266    2198 profile.go:143] Saving config to /home/joji/.minikube/profiles/minikube/config.json ...
I0721 20:40:33.712706    2198 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e in local docker daemon, skipping pull
I0721 20:40:33.712717    2198 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e exists in daemon, skipping load
I0721 20:40:33.712747    2198 cache.go:194] Successfully downloaded all kic artifacts
I0721 20:40:33.712772    2198 start.go:360] acquireMachinesLock for minikube: {Name:mkc0ce9b1c8191244edea6f10cd44ba7fe7aa2f5 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0721 20:40:33.712869    2198 start.go:364] duration metric: took 83.908¬µs to acquireMachinesLock for "minikube"
I0721 20:40:33.712882    2198 start.go:96] Skipping create...Using existing machine configuration
I0721 20:40:33.712885    2198 fix.go:54] fixHost starting: 
I0721 20:40:33.713055    2198 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0721 20:40:33.737265    2198 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0721 20:40:33.737329    2198 fix.go:138] unexpected machine state, will restart: <nil>
I0721 20:40:33.740256    2198 out.go:177] üîÑ  Restarting existing docker container for "minikube" ...
I0721 20:40:33.743114    2198 cli_runner.go:164] Run: docker start minikube
I0721 20:40:34.062589    2198 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0721 20:40:34.095762    2198 kic.go:430] container "minikube" state is running.
I0721 20:40:34.096224    2198 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0721 20:40:34.116536    2198 profile.go:143] Saving config to /home/joji/.minikube/profiles/minikube/config.json ...
I0721 20:40:34.116695    2198 machine.go:94] provisionDockerMachine start ...
I0721 20:40:34.116746    2198 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0721 20:40:34.139036    2198 main.go:141] libmachine: Using SSH client type: native
I0721 20:40:34.139627    2198 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 62882 <nil> <nil>}
I0721 20:40:34.139661    2198 main.go:141] libmachine: About to run SSH command:
hostname
I0721 20:40:34.140335    2198 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0721 20:40:37.277424    2198 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0721 20:40:37.277437    2198 ubuntu.go:169] provisioning hostname "minikube"
I0721 20:40:37.277497    2198 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0721 20:40:37.303177    2198 main.go:141] libmachine: Using SSH client type: native
I0721 20:40:37.303302    2198 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 62882 <nil> <nil>}
I0721 20:40:37.303307    2198 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0721 20:40:37.467758    2198 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0721 20:40:37.467814    2198 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0721 20:40:37.491540    2198 main.go:141] libmachine: Using SSH client type: native
I0721 20:40:37.491672    2198 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 62882 <nil> <nil>}
I0721 20:40:37.491681    2198 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0721 20:40:37.636261    2198 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0721 20:40:37.636283    2198 ubuntu.go:175] set auth options {CertDir:/home/joji/.minikube CaCertPath:/home/joji/.minikube/certs/ca.pem CaPrivateKeyPath:/home/joji/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/joji/.minikube/machines/server.pem ServerKeyPath:/home/joji/.minikube/machines/server-key.pem ClientKeyPath:/home/joji/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/joji/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/joji/.minikube}
I0721 20:40:37.636294    2198 ubuntu.go:177] setting up certificates
I0721 20:40:37.636300    2198 provision.go:84] configureAuth start
I0721 20:40:37.636361    2198 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0721 20:40:37.663947    2198 provision.go:143] copyHostCerts
I0721 20:40:37.664373    2198 exec_runner.go:144] found /home/joji/.minikube/ca.pem, removing ...
I0721 20:40:37.664384    2198 exec_runner.go:203] rm: /home/joji/.minikube/ca.pem
I0721 20:40:37.664422    2198 exec_runner.go:151] cp: /home/joji/.minikube/certs/ca.pem --> /home/joji/.minikube/ca.pem (1074 bytes)
I0721 20:40:37.664941    2198 exec_runner.go:144] found /home/joji/.minikube/cert.pem, removing ...
I0721 20:40:37.664947    2198 exec_runner.go:203] rm: /home/joji/.minikube/cert.pem
I0721 20:40:37.664972    2198 exec_runner.go:151] cp: /home/joji/.minikube/certs/cert.pem --> /home/joji/.minikube/cert.pem (1115 bytes)
I0721 20:40:37.665399    2198 exec_runner.go:144] found /home/joji/.minikube/key.pem, removing ...
I0721 20:40:37.665404    2198 exec_runner.go:203] rm: /home/joji/.minikube/key.pem
I0721 20:40:37.665430    2198 exec_runner.go:151] cp: /home/joji/.minikube/certs/key.pem --> /home/joji/.minikube/key.pem (1675 bytes)
I0721 20:40:37.665822    2198 provision.go:117] generating server cert: /home/joji/.minikube/machines/server.pem ca-key=/home/joji/.minikube/certs/ca.pem private-key=/home/joji/.minikube/certs/ca-key.pem org=joji.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0721 20:40:37.964950    2198 provision.go:177] copyRemoteCerts
I0721 20:40:37.965022    2198 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0721 20:40:37.965090    2198 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0721 20:40:37.984551    2198 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:62882 SSHKeyPath:/home/joji/.minikube/machines/minikube/id_rsa Username:docker}
I0721 20:40:38.090300    2198 ssh_runner.go:362] scp /home/joji/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0721 20:40:38.116076    2198 ssh_runner.go:362] scp /home/joji/.minikube/machines/server.pem --> /etc/docker/server.pem (1176 bytes)
I0721 20:40:38.136778    2198 ssh_runner.go:362] scp /home/joji/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0721 20:40:38.158439    2198 provision.go:87] duration metric: took 522.129829ms to configureAuth
I0721 20:40:38.158451    2198 ubuntu.go:193] setting minikube options for container-runtime
I0721 20:40:38.158561    2198 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0721 20:40:38.158602    2198 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0721 20:40:38.182869    2198 main.go:141] libmachine: Using SSH client type: native
I0721 20:40:38.182990    2198 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 62882 <nil> <nil>}
I0721 20:40:38.182994    2198 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0721 20:40:38.317874    2198 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0721 20:40:38.317882    2198 ubuntu.go:71] root file system type: overlay
I0721 20:40:38.317942    2198 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0721 20:40:38.317988    2198 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0721 20:40:38.339892    2198 main.go:141] libmachine: Using SSH client type: native
I0721 20:40:38.340029    2198 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 62882 <nil> <nil>}
I0721 20:40:38.340075    2198 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0721 20:40:38.495714    2198 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0721 20:40:38.495768    2198 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0721 20:40:38.517114    2198 main.go:141] libmachine: Using SSH client type: native
I0721 20:40:38.517267    2198 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 62882 <nil> <nil>}
I0721 20:40:38.517277    2198 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0721 20:40:38.664291    2198 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0721 20:40:38.664302    2198 machine.go:97] duration metric: took 4.547600807s to provisionDockerMachine
I0721 20:40:38.664309    2198 start.go:293] postStartSetup for "minikube" (driver="docker")
I0721 20:40:38.664317    2198 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0721 20:40:38.664364    2198 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0721 20:40:38.664400    2198 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0721 20:40:38.690692    2198 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:62882 SSHKeyPath:/home/joji/.minikube/machines/minikube/id_rsa Username:docker}
I0721 20:40:38.802267    2198 ssh_runner.go:195] Run: cat /etc/os-release
I0721 20:40:38.806166    2198 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0721 20:40:38.806182    2198 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0721 20:40:38.806187    2198 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0721 20:40:38.806191    2198 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0721 20:40:38.806198    2198 filesync.go:126] Scanning /home/joji/.minikube/addons for local assets ...
I0721 20:40:38.806686    2198 filesync.go:126] Scanning /home/joji/.minikube/files for local assets ...
I0721 20:40:38.807110    2198 start.go:296] duration metric: took 142.794967ms for postStartSetup
I0721 20:40:38.807154    2198 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0721 20:40:38.807188    2198 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0721 20:40:38.829325    2198 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:62882 SSHKeyPath:/home/joji/.minikube/machines/minikube/id_rsa Username:docker}
I0721 20:40:38.927561    2198 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0721 20:40:38.933292    2198 fix.go:56] duration metric: took 5.220403099s for fixHost
I0721 20:40:38.933305    2198 start.go:83] releasing machines lock for "minikube", held for 5.220429305s
I0721 20:40:38.933363    2198 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0721 20:40:38.959986    2198 ssh_runner.go:195] Run: cat /version.json
I0721 20:40:38.960023    2198 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0721 20:40:38.960106    2198 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0721 20:40:38.960149    2198 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0721 20:40:38.986256    2198 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:62882 SSHKeyPath:/home/joji/.minikube/machines/minikube/id_rsa Username:docker}
I0721 20:40:38.987971    2198 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:62882 SSHKeyPath:/home/joji/.minikube/machines/minikube/id_rsa Username:docker}
I0721 20:40:39.086698    2198 ssh_runner.go:195] Run: systemctl --version
I0721 20:40:39.340252    2198 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0721 20:40:39.344897    2198 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0721 20:40:39.363702    2198 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0721 20:40:39.363748    2198 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0721 20:40:39.372187    2198 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0721 20:40:39.372200    2198 start.go:494] detecting cgroup driver to use...
I0721 20:40:39.372221    2198 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0721 20:40:39.372306    2198 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0721 20:40:39.387092    2198 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0721 20:40:39.397269    2198 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0721 20:40:39.406432    2198 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0721 20:40:39.406475    2198 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0721 20:40:39.415581    2198 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0721 20:40:39.425046    2198 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0721 20:40:39.434156    2198 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0721 20:40:39.443182    2198 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0721 20:40:39.451605    2198 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0721 20:40:39.460609    2198 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0721 20:40:39.469720    2198 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0721 20:40:39.478971    2198 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0721 20:40:39.489061    2198 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0721 20:40:39.497377    2198 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0721 20:40:39.604470    2198 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0721 20:40:39.732172    2198 start.go:494] detecting cgroup driver to use...
I0721 20:40:39.732199    2198 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0721 20:40:39.732239    2198 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0721 20:40:39.743556    2198 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0721 20:40:39.743602    2198 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0721 20:40:39.757330    2198 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0721 20:40:39.776366    2198 ssh_runner.go:195] Run: which cri-dockerd
I0721 20:40:39.779980    2198 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0721 20:40:39.788832    2198 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0721 20:40:39.806321    2198 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0721 20:40:39.931827    2198 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0721 20:40:40.028799    2198 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0721 20:40:40.028869    2198 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0721 20:40:40.046382    2198 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0721 20:40:40.136973    2198 ssh_runner.go:195] Run: sudo systemctl restart docker
I0721 20:40:40.441451    2198 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0721 20:40:40.451780    2198 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0721 20:40:40.463668    2198 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0721 20:40:40.474059    2198 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0721 20:40:40.575559    2198 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0721 20:40:40.664725    2198 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0721 20:40:40.754893    2198 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0721 20:40:40.767411    2198 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0721 20:40:40.777755    2198 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0721 20:40:40.887930    2198 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0721 20:40:41.246771    2198 start.go:541] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0721 20:40:41.246822    2198 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0721 20:40:41.250524    2198 start.go:562] Will wait 60s for crictl version
I0721 20:40:41.250574    2198 ssh_runner.go:195] Run: which crictl
I0721 20:40:41.254143    2198 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0721 20:40:41.438928    2198 start.go:578] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  26.1.1
RuntimeApiVersion:  v1
I0721 20:40:41.438974    2198 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0721 20:40:41.596004    2198 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0721 20:40:41.624601    2198 out.go:204] üê≥  Preparing Kubernetes v1.30.0 on Docker 26.1.1 ...
I0721 20:40:41.624712    2198 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0721 20:40:41.645901    2198 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0721 20:40:41.649770    2198 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0721 20:40:41.659782    2198 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0721 20:40:41.684269    2198 kubeadm.go:877] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/joji:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0721 20:40:41.684339    2198 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0721 20:40:41.684388    2198 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0721 20:40:41.701938    2198 docker.go:685] Got preloaded images: -- stdout --
aureezzhenx/nginx-curl:latest
aureezzhenx/nodejs-cronjob:latest
nginx:latest
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0721 20:40:41.701948    2198 docker.go:615] Images already preloaded, skipping extraction
I0721 20:40:41.701998    2198 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0721 20:40:41.721872    2198 docker.go:685] Got preloaded images: -- stdout --
aureezzhenx/nginx-curl:latest
aureezzhenx/nodejs-cronjob:latest
nginx:latest
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0721 20:40:41.721881    2198 cache_images.go:84] Images are preloaded, skipping loading
I0721 20:40:41.721887    2198 kubeadm.go:928] updating node { 192.168.49.2 8443 v1.30.0 docker true true} ...
I0721 20:40:41.721969    2198 kubeadm.go:940] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.30.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0721 20:40:41.722015    2198 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0721 20:40:42.017517    2198 cni.go:84] Creating CNI manager for ""
I0721 20:40:42.017528    2198 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0721 20:40:42.017534    2198 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0721 20:40:42.017545    2198 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.30.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0721 20:40:42.017635    2198 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.30.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0721 20:40:42.017677    2198 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.30.0
I0721 20:40:42.028342    2198 binaries.go:44] Found k8s binaries, skipping transfer
I0721 20:40:42.028385    2198 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0721 20:40:42.036545    2198 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0721 20:40:42.053363    2198 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0721 20:40:42.069326    2198 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2150 bytes)
I0721 20:40:42.086345    2198 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0721 20:40:42.089880    2198 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0721 20:40:42.100282    2198 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0721 20:40:42.204165    2198 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0721 20:40:42.216780    2198 certs.go:68] Setting up /home/joji/.minikube/profiles/minikube for IP: 192.168.49.2
I0721 20:40:42.216787    2198 certs.go:194] generating shared ca certs ...
I0721 20:40:42.216796    2198 certs.go:226] acquiring lock for ca certs: {Name:mkab972b282fcf55072d5c9ed301bf6c546b1d40 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0721 20:40:42.216949    2198 certs.go:235] skipping valid "minikubeCA" ca cert: /home/joji/.minikube/ca.key
I0721 20:40:42.217549    2198 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/joji/.minikube/proxy-client-ca.key
I0721 20:40:42.217557    2198 certs.go:256] generating profile certs ...
I0721 20:40:42.217642    2198 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /home/joji/.minikube/profiles/minikube/client.key
I0721 20:40:42.218292    2198 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /home/joji/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0721 20:40:42.218599    2198 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /home/joji/.minikube/profiles/minikube/proxy-client.key
I0721 20:40:42.218703    2198 certs.go:484] found cert: /home/joji/.minikube/certs/ca-key.pem (1679 bytes)
I0721 20:40:42.218724    2198 certs.go:484] found cert: /home/joji/.minikube/certs/ca.pem (1074 bytes)
I0721 20:40:42.218743    2198 certs.go:484] found cert: /home/joji/.minikube/certs/cert.pem (1115 bytes)
I0721 20:40:42.218758    2198 certs.go:484] found cert: /home/joji/.minikube/certs/key.pem (1675 bytes)
I0721 20:40:42.219144    2198 ssh_runner.go:362] scp /home/joji/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0721 20:40:42.242876    2198 ssh_runner.go:362] scp /home/joji/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0721 20:40:42.265734    2198 ssh_runner.go:362] scp /home/joji/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0721 20:40:42.288807    2198 ssh_runner.go:362] scp /home/joji/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0721 20:40:42.310712    2198 ssh_runner.go:362] scp /home/joji/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0721 20:40:42.333871    2198 ssh_runner.go:362] scp /home/joji/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0721 20:40:42.357302    2198 ssh_runner.go:362] scp /home/joji/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0721 20:40:42.380771    2198 ssh_runner.go:362] scp /home/joji/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0721 20:40:42.403779    2198 ssh_runner.go:362] scp /home/joji/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0721 20:40:42.429672    2198 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0721 20:40:42.447804    2198 ssh_runner.go:195] Run: openssl version
I0721 20:40:42.461120    2198 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0721 20:40:42.473069    2198 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0721 20:40:42.476859    2198 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Jul 20 06:02 /usr/share/ca-certificates/minikubeCA.pem
I0721 20:40:42.476899    2198 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0721 20:40:42.483583    2198 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0721 20:40:42.492451    2198 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0721 20:40:42.498275    2198 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0721 20:40:42.505632    2198 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0721 20:40:42.512033    2198 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0721 20:40:42.518979    2198 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0721 20:40:42.526046    2198 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0721 20:40:42.532725    2198 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0721 20:40:42.539772    2198 kubeadm.go:391] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/joji:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0721 20:40:42.539910    2198 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0721 20:40:42.555895    2198 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
W0721 20:40:42.565578    2198 kubeadm.go:404] apiserver tunnel failed: apiserver port not set
I0721 20:40:42.565587    2198 kubeadm.go:407] found existing configuration files, will attempt cluster restart
I0721 20:40:42.565595    2198 kubeadm.go:587] restartPrimaryControlPlane start ...
I0721 20:40:42.565630    2198 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0721 20:40:42.575966    2198 kubeadm.go:129] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0721 20:40:42.576026    2198 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0721 20:40:42.598767    2198 kubeconfig.go:125] found "minikube" server: "https://127.0.0.1:60738"
I0721 20:40:42.598785    2198 kubeconfig.go:47] verify endpoint returned: got: 127.0.0.1:60738, want: 127.0.0.1:62881
I0721 20:40:42.599290    2198 kubeconfig.go:62] /home/joji/.kube/config needs updating (will repair): [kubeconfig needs server address update]
I0721 20:40:42.599803    2198 lock.go:35] WriteFile acquiring /home/joji/.kube/config: {Name:mk436f751372566f5950fc246a2d81d2ef15a34c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0721 20:40:42.625560    2198 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0721 20:40:42.634019    2198 kubeadm.go:624] The running cluster does not require reconfiguration: 127.0.0.1
I0721 20:40:42.634034    2198 kubeadm.go:591] duration metric: took 68.435496ms to restartPrimaryControlPlane
I0721 20:40:42.634038    2198 kubeadm.go:393] duration metric: took 94.273267ms to StartCluster
I0721 20:40:42.634048    2198 settings.go:142] acquiring lock: {Name:mk35efaabc08542823301d245afa765220857400 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0721 20:40:42.634139    2198 settings.go:150] Updating kubeconfig:  /home/joji/.kube/config
I0721 20:40:42.634952    2198 lock.go:35] WriteFile acquiring /home/joji/.kube/config: {Name:mk436f751372566f5950fc246a2d81d2ef15a34c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0721 20:40:42.635175    2198 start.go:234] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0721 20:40:42.640644    2198 out.go:177] üîé  Verifying Kubernetes components...
I0721 20:40:42.635274    2198 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0721 20:40:42.635281    2198 addons.go:502] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false yakd:false]
I0721 20:40:42.640814    2198 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0721 20:40:42.640827    2198 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0721 20:40:42.640838    2198 addons.go:234] Setting addon storage-provisioner=true in "minikube"
I0721 20:40:42.640842    2198 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
W0721 20:40:42.640843    2198 addons.go:243] addon storage-provisioner should already be in state true
I0721 20:40:42.640862    2198 host.go:66] Checking if "minikube" exists ...
I0721 20:40:42.641091    2198 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0721 20:40:42.641110    2198 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0721 20:40:42.645977    2198 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0721 20:40:42.681588    2198 addons.go:234] Setting addon default-storageclass=true in "minikube"
W0721 20:40:42.681596    2198 addons.go:243] addon default-storageclass should already be in state true
I0721 20:40:42.681612    2198 host.go:66] Checking if "minikube" exists ...
I0721 20:40:42.681975    2198 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0721 20:40:42.686891    2198 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0721 20:40:42.691530    2198 addons.go:426] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0721 20:40:42.691539    2198 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0721 20:40:42.691587    2198 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0721 20:40:42.723845    2198 addons.go:426] installing /etc/kubernetes/addons/storageclass.yaml
I0721 20:40:42.723858    2198 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0721 20:40:42.723913    2198 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0721 20:40:42.727893    2198 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:62882 SSHKeyPath:/home/joji/.minikube/machines/minikube/id_rsa Username:docker}
I0721 20:40:42.748716    2198 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:62882 SSHKeyPath:/home/joji/.minikube/machines/minikube/id_rsa Username:docker}
I0721 20:40:42.756610    2198 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0721 20:40:42.796188    2198 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0721 20:40:42.821501    2198 api_server.go:52] waiting for apiserver process to appear ...
I0721 20:40:42.821548    2198 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0721 20:40:42.894050    2198 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0721 20:40:42.907636    2198 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
W0721 20:40:43.185768    2198 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0721 20:40:43.185782    2198 retry.go:31] will retry after 239.158012ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0721 20:40:43.185838    2198 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0721 20:40:43.185846    2198 retry.go:31] will retry after 136.165369ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0721 20:40:43.322562    2198 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0721 20:40:43.322604    2198 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0721 20:40:43.379840    2198 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0721 20:40:43.379854    2198 retry.go:31] will retry after 480.275323ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0721 20:40:43.426185    2198 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0721 20:40:43.524351    2198 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0721 20:40:43.524367    2198 retry.go:31] will retry after 370.065778ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0721 20:40:43.821815    2198 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0721 20:40:43.832487    2198 api_server.go:72] duration metric: took 1.197294048s to wait for apiserver process to appear ...
I0721 20:40:43.832498    2198 api_server.go:88] waiting for apiserver healthz status ...
I0721 20:40:43.832509    2198 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:62881/healthz ...
I0721 20:40:43.833200    2198 api_server.go:269] stopped: https://127.0.0.1:62881/healthz: Get "https://127.0.0.1:62881/healthz": EOF
I0721 20:40:43.860530    2198 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0721 20:40:43.895565    2198 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0721 20:40:43.917241    2198 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0721 20:40:43.917256    2198 retry.go:31] will retry after 282.60023ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0721 20:40:43.954512    2198 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0721 20:40:43.954533    2198 retry.go:31] will retry after 697.36099ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0721 20:40:44.201008    2198 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0721 20:40:44.258241    2198 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0721 20:40:44.258299    2198 retry.go:31] will retry after 1.091620363s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0721 20:40:44.333537    2198 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:62881/healthz ...
I0721 20:40:44.334402    2198 api_server.go:269] stopped: https://127.0.0.1:62881/healthz: Get "https://127.0.0.1:62881/healthz": EOF
I0721 20:40:44.652847    2198 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0721 20:40:44.710609    2198 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0721 20:40:44.710624    2198 retry.go:31] will retry after 1.029544963s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0721 20:40:44.832875    2198 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:62881/healthz ...
I0721 20:40:45.350588    2198 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0721 20:40:45.740427    2198 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0721 20:40:46.631562    2198 api_server.go:279] https://127.0.0.1:62881/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0721 20:40:46.631576    2198 api_server.go:103] status: https://127.0.0.1:62881/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0721 20:40:46.631585    2198 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:62881/healthz ...
I0721 20:40:46.749942    2198 api_server.go:279] https://127.0.0.1:62881/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0721 20:40:46.749959    2198 api_server.go:103] status: https://127.0.0.1:62881/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0721 20:40:46.833145    2198 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:62881/healthz ...
I0721 20:40:46.837074    2198 api_server.go:279] https://127.0.0.1:62881/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0721 20:40:46.837084    2198 api_server.go:103] status: https://127.0.0.1:62881/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0721 20:40:47.126213    2198 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (1.775604436s)
I0721 20:40:47.333383    2198 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:62881/healthz ...
I0721 20:40:47.337493    2198 api_server.go:279] https://127.0.0.1:62881/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0721 20:40:47.337506    2198 api_server.go:103] status: https://127.0.0.1:62881/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0721 20:40:47.804941    2198 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (2.064497303s)
I0721 20:40:47.833100    2198 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:62881/healthz ...
I0721 20:40:48.043818    2198 out.go:177] üåü  Enabled addons: default-storageclass, storage-provisioner
I0721 20:40:48.047361    2198 api_server.go:279] https://127.0.0.1:62881/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0721 20:40:48.133000    2198 api_server.go:103] status: https://127.0.0.1:62881/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0721 20:40:48.133003    2198 addons.go:505] duration metric: took 5.497717633s for enable addons: enabled=[default-storageclass storage-provisioner]
I0721 20:40:48.333057    2198 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:62881/healthz ...
I0721 20:40:48.385521    2198 api_server.go:279] https://127.0.0.1:62881/healthz returned 200:
ok
I0721 20:40:48.386686    2198 api_server.go:141] control plane version: v1.30.0
I0721 20:40:48.386695    2198 api_server.go:131] duration metric: took 4.554193426s to wait for apiserver health ...
I0721 20:40:48.386706    2198 system_pods.go:43] waiting for kube-system pods to appear ...
I0721 20:40:48.393018    2198 system_pods.go:59] 7 kube-system pods found
I0721 20:40:48.393030    2198 system_pods.go:61] "coredns-7db6d8ff4d-jwqcj" [c4846a43-b67d-4c5f-b780-8a13472cc79a] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0721 20:40:48.393035    2198 system_pods.go:61] "etcd-minikube" [87805611-560d-4be7-8ee4-e49ad3dc2118] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0721 20:40:48.393038    2198 system_pods.go:61] "kube-apiserver-minikube" [e21aac0f-3564-4d69-ac0f-7fbee99a0912] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0721 20:40:48.393042    2198 system_pods.go:61] "kube-controller-manager-minikube" [e483eaa2-3e5d-4f90-8165-e7afd2e6b8bc] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0721 20:40:48.393045    2198 system_pods.go:61] "kube-proxy-xt2m7" [8f1ec671-1fef-4900-abf4-e2a3022388aa] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0721 20:40:48.393047    2198 system_pods.go:61] "kube-scheduler-minikube" [443bfd82-0817-4642-b2c5-0d6e04abc740] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0721 20:40:48.393049    2198 system_pods.go:61] "storage-provisioner" [484b00d1-2780-4c58-988e-209f09c5789e] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0721 20:40:48.393054    2198 system_pods.go:74] duration metric: took 6.344041ms to wait for pod list to return data ...
I0721 20:40:48.393060    2198 kubeadm.go:576] duration metric: took 5.757871721s to wait for: map[apiserver:true system_pods:true]
I0721 20:40:48.393068    2198 node_conditions.go:102] verifying NodePressure condition ...
I0721 20:40:48.407981    2198 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0721 20:40:48.407998    2198 node_conditions.go:123] node cpu capacity is 6
I0721 20:40:48.408011    2198 node_conditions.go:105] duration metric: took 14.939738ms to run NodePressure ...
I0721 20:40:48.408020    2198 start.go:240] waiting for startup goroutines ...
I0721 20:40:48.408025    2198 start.go:245] waiting for cluster config update ...
I0721 20:40:48.408032    2198 start.go:254] writing updated cluster config ...
I0721 20:40:48.408197    2198 ssh_runner.go:195] Run: rm -f paused
I0721 20:40:48.627065    2198 start.go:600] kubectl: 1.30.3, cluster: 1.30.0 (minor skew: 0)
I0721 20:40:48.711953    2198 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Jul 21 13:40:40 minikube dockerd[756]: time="2024-07-21T13:40:40.148333220Z" level=info msg="Daemon shutdown complete"
Jul 21 13:40:40 minikube systemd[1]: docker.service: Deactivated successfully.
Jul 21 13:40:40 minikube systemd[1]: Stopped Docker Application Container Engine.
Jul 21 13:40:40 minikube systemd[1]: Starting Docker Application Container Engine...
Jul 21 13:40:40 minikube dockerd[990]: time="2024-07-21T13:40:40.195192979Z" level=info msg="Starting up"
Jul 21 13:40:40 minikube dockerd[990]: time="2024-07-21T13:40:40.214257948Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Jul 21 13:40:40 minikube dockerd[990]: time="2024-07-21T13:40:40.228964759Z" level=info msg="Loading containers: start."
Jul 21 13:40:40 minikube dockerd[990]: time="2024-07-21T13:40:40.359436950Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Jul 21 13:40:40 minikube dockerd[990]: time="2024-07-21T13:40:40.394250507Z" level=info msg="Loading containers: done."
Jul 21 13:40:40 minikube dockerd[990]: time="2024-07-21T13:40:40.405802278Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
Jul 21 13:40:40 minikube dockerd[990]: time="2024-07-21T13:40:40.405842389Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
Jul 21 13:40:40 minikube dockerd[990]: time="2024-07-21T13:40:40.405850391Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Jul 21 13:40:40 minikube dockerd[990]: time="2024-07-21T13:40:40.405854792Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Jul 21 13:40:40 minikube dockerd[990]: time="2024-07-21T13:40:40.405870196Z" level=info msg="Docker daemon" commit=ac2de55 containerd-snapshotter=false storage-driver=overlay2 version=26.1.1
Jul 21 13:40:40 minikube dockerd[990]: time="2024-07-21T13:40:40.405910307Z" level=info msg="Daemon has completed initialization"
Jul 21 13:40:40 minikube dockerd[990]: time="2024-07-21T13:40:40.439581260Z" level=info msg="API listen on /var/run/docker.sock"
Jul 21 13:40:40 minikube dockerd[990]: time="2024-07-21T13:40:40.439647277Z" level=info msg="API listen on [::]:2376"
Jul 21 13:40:40 minikube systemd[1]: Started Docker Application Container Engine.
Jul 21 13:40:40 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Jul 21 13:40:41 minikube cri-dockerd[1227]: time="2024-07-21T13:40:41Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Jul 21 13:40:41 minikube cri-dockerd[1227]: time="2024-07-21T13:40:41Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Jul 21 13:40:41 minikube cri-dockerd[1227]: time="2024-07-21T13:40:41Z" level=info msg="Start docker client with request timeout 0s"
Jul 21 13:40:41 minikube cri-dockerd[1227]: time="2024-07-21T13:40:41Z" level=info msg="Hairpin mode is set to hairpin-veth"
Jul 21 13:40:41 minikube cri-dockerd[1227]: time="2024-07-21T13:40:41Z" level=info msg="Loaded network plugin cni"
Jul 21 13:40:41 minikube cri-dockerd[1227]: time="2024-07-21T13:40:41Z" level=info msg="Docker cri networking managed by network plugin cni"
Jul 21 13:40:41 minikube cri-dockerd[1227]: time="2024-07-21T13:40:41Z" level=info msg="Setting cgroupDriver cgroupfs"
Jul 21 13:40:41 minikube cri-dockerd[1227]: time="2024-07-21T13:40:41Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Jul 21 13:40:41 minikube cri-dockerd[1227]: time="2024-07-21T13:40:41Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Jul 21 13:40:41 minikube cri-dockerd[1227]: time="2024-07-21T13:40:41Z" level=info msg="Start cri-dockerd grpc backend"
Jul 21 13:40:41 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Jul 21 13:40:42 minikube cri-dockerd[1227]: time="2024-07-21T13:40:42Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-7db6d8ff4d-jwqcj_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"5aa2ff600cb7ff140b0f7bc11da0bc6f1e1a07c10e0d3f5a2582472fe6c1d89d\""
Jul 21 13:40:43 minikube cri-dockerd[1227]: time="2024-07-21T13:40:43Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f5246507fe7e5f8f89347a1690e3e3db62d0e69d6de2405328980a883a404155/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jul 21 13:40:43 minikube cri-dockerd[1227]: time="2024-07-21T13:40:43Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/058efb2ab9d28e19530de574d53fe46edfc2002fe7244be2785596ee6cfdfcb3/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jul 21 13:40:43 minikube cri-dockerd[1227]: time="2024-07-21T13:40:43Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/83f1ec5da411d7e0491c24b44bc921ec64752353204592cc10abde70fb2e4210/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jul 21 13:40:43 minikube cri-dockerd[1227]: time="2024-07-21T13:40:43Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5416df5ffdc5d396aaa513f11bd74296ab9a0693eee3d19f0554f679d3b41727/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jul 21 13:40:47 minikube cri-dockerd[1227]: time="2024-07-21T13:40:47Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Jul 21 13:40:49 minikube cri-dockerd[1227]: time="2024-07-21T13:40:49Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b0e41b4f490674c41ae6092f9b8071a26e1a83efc55ac6ec99b18e9bed5acd78/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jul 21 13:40:49 minikube cri-dockerd[1227]: time="2024-07-21T13:40:49Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5eb24b1d1107c1d9e0f06598fba89a88fdc2fc598e00017acaa23c641ca5ee6d/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jul 21 13:40:50 minikube cri-dockerd[1227]: time="2024-07-21T13:40:50Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ddc8e205a6f4a085a7a2449a08bcb09062517ef03e60f5c0b47a5c4a30d7dc67/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jul 21 13:40:53 minikube dockerd[990]: time="2024-07-21T13:40:53.389702891Z" level=info msg="ignoring event" container=88da2a63a28198b4bdea55be0e1932a961b3bb931b54412bdf500919a90f258b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 21 13:43:30 minikube cri-dockerd[1227]: time="2024-07-21T13:43:30Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/48094a285bee280f7aebe9403274d78155803b58f7b00cea8fe24f16bf1b1118/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 21 13:43:30 minikube cri-dockerd[1227]: time="2024-07-21T13:43:30Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/60f3e84fed6c1f73a030a6d965d0cab7f5b04967bbb26833f3f60b8c80c5d3b1/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 21 13:43:30 minikube cri-dockerd[1227]: time="2024-07-21T13:43:30Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/95b4cc61cfa4544f9593ddb542a54820fb71b25c21514ec98cf68b60d9aced06/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 21 13:43:34 minikube cri-dockerd[1227]: time="2024-07-21T13:43:34Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Jul 21 13:43:38 minikube cri-dockerd[1227]: time="2024-07-21T13:43:38Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Jul 21 13:43:43 minikube cri-dockerd[1227]: time="2024-07-21T13:43:43Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Jul 21 13:48:14 minikube dockerd[990]: 2024/07/21 13:48:14 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 21 13:48:14 minikube dockerd[990]: 2024/07/21 13:48:14 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 21 13:48:15 minikube dockerd[990]: 2024/07/21 13:48:15 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 21 13:48:15 minikube dockerd[990]: 2024/07/21 13:48:15 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 21 13:48:15 minikube dockerd[990]: 2024/07/21 13:48:15 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 21 13:48:15 minikube dockerd[990]: 2024/07/21 13:48:15 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 21 13:48:15 minikube dockerd[990]: 2024/07/21 13:48:15 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 21 13:48:15 minikube dockerd[990]: 2024/07/21 13:48:15 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 21 13:48:15 minikube dockerd[990]: 2024/07/21 13:48:15 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 21 13:48:15 minikube dockerd[990]: 2024/07/21 13:48:15 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 21 13:48:15 minikube dockerd[990]: 2024/07/21 13:48:15 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 21 13:48:15 minikube dockerd[990]: 2024/07/21 13:48:15 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 21 13:48:15 minikube dockerd[990]: 2024/07/21 13:48:15 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 21 13:48:15 minikube dockerd[990]: 2024/07/21 13:48:15 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)


==> container status <==
CONTAINER           IMAGE                                                                           CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
a267745c390e4       nginx@sha256:67682bda769fae1ccf5183192b8daf37b64cae99c6c3302650f6f8bf5f0f95df   4 minutes ago       Running             nginx                     0                   95b4cc61cfa45       nginx-rs-match-expression-nlvcz
3c95051dbc67e       nginx@sha256:67682bda769fae1ccf5183192b8daf37b64cae99c6c3302650f6f8bf5f0f95df   4 minutes ago       Running             nginx                     0                   60f3e84fed6c1       nginx-rs-match-expression-9f7fr
63834191ec259       nginx@sha256:67682bda769fae1ccf5183192b8daf37b64cae99c6c3302650f6f8bf5f0f95df   4 minutes ago       Running             nginx                     0                   48094a285bee2       nginx-rs-match-expression-htrzc
f6ace3b4a0f55       6e38f40d628db                                                                   7 minutes ago       Running             storage-provisioner       4                   5eb24b1d1107c       storage-provisioner
6d741091a3fd3       cbb01a7bd410d                                                                   7 minutes ago       Running             coredns                   1                   ddc8e205a6f4a       coredns-7db6d8ff4d-jwqcj
44100b19f8790       a0bf559e280cf                                                                   7 minutes ago       Running             kube-proxy                1                   b0e41b4f49067       kube-proxy-xt2m7
88da2a63a2819       6e38f40d628db                                                                   7 minutes ago       Exited              storage-provisioner       3                   5eb24b1d1107c       storage-provisioner
6e298e10f0286       259c8277fcbbc                                                                   7 minutes ago       Running             kube-scheduler            1                   f5246507fe7e5       kube-scheduler-minikube
68fa5ec968726       c7aad43836fa5                                                                   7 minutes ago       Running             kube-controller-manager   1                   5416df5ffdc5d       kube-controller-manager-minikube
f2cabc1cbfd1d       c42f13656d0b2                                                                   7 minutes ago       Running             kube-apiserver            1                   058efb2ab9d28       kube-apiserver-minikube
fa8dfc74eb535       3861cfcd7c04c                                                                   7 minutes ago       Running             etcd                      1                   83f1ec5da411d       etcd-minikube
563d5d01a2ee0       cbb01a7bd410d                                                                   4 hours ago         Exited              coredns                   0                   5aa2ff600cb7f       coredns-7db6d8ff4d-jwqcj
505681e92107b       a0bf559e280cf                                                                   4 hours ago         Exited              kube-proxy                0                   dc8f7e518cbb9       kube-proxy-xt2m7
ffd6880610266       259c8277fcbbc                                                                   4 hours ago         Exited              kube-scheduler            0                   7fc9b4312b297       kube-scheduler-minikube
3067d6a45f941       3861cfcd7c04c                                                                   4 hours ago         Exited              etcd                      0                   6d2bc80c6c1f0       etcd-minikube
92545170e0c23       c42f13656d0b2                                                                   4 hours ago         Exited              kube-apiserver            0                   ab45613d740c5       kube-apiserver-minikube
2699cfb8219cd       c7aad43836fa5                                                                   4 hours ago         Exited              kube-controller-manager   0                   ead8fbcb37105       kube-controller-manager-minikube


==> coredns [563d5d01a2ee] <==
[INFO] 10.244.0.21:47235 - 16838 "AAAA IN kubernetes-services.default.svc.cluster.local.default.svc.cluster.local. udp 100 false 1232" NXDOMAIN qr,aa,rd 182 0.000077603s
[INFO] 10.244.0.21:47235 - 22955 "A IN kubernetes-services.default.svc.cluster.local.default.svc.cluster.local. udp 100 false 1232" NXDOMAIN qr,aa,rd 182 0.000068003s
[INFO] 10.244.0.21:47235 - 40058 "AAAA IN kubernetes-services.default.svc.cluster.local.svc.cluster.local. udp 92 false 1232" NXDOMAIN qr,aa,rd 174 0.000047201s
[INFO] 10.244.0.21:47235 - 15723 "A IN kubernetes-services.default.svc.cluster.local.svc.cluster.local. udp 92 false 1232" NXDOMAIN qr,aa,rd 174 0.000051302s
[INFO] 10.244.0.21:47235 - 50623 "AAAA IN kubernetes-services.default.svc.cluster.local.cluster.local. udp 88 false 1232" NXDOMAIN qr,aa,rd 170 0.000044302s
[INFO] 10.244.0.21:47235 - 46307 "A IN kubernetes-services.default.svc.cluster.local.cluster.local. udp 88 false 1232" NXDOMAIN qr,aa,rd 170 0.000037702s
[INFO] 10.244.0.21:47235 - 16337 "A IN kubernetes-services.default.svc.cluster.local. udp 74 false 1232" NXDOMAIN qr,aa,rd 156 0.000054503s
[INFO] 10.244.0.21:47235 - 14176 "AAAA IN kubernetes-services.default.svc.cluster.local. udp 74 false 1232" NXDOMAIN qr,aa,rd 156 0.000040501s
[INFO] 10.244.0.21:46088 - 18790 "AAAA IN kubernetes-services.default.svc.cluster.local.default.svc.cluster.local. udp 100 false 1232" NXDOMAIN qr,aa,rd 182 0.000096004s
[INFO] 10.244.0.21:46088 - 64243 "A IN kubernetes-services.default.svc.cluster.local.default.svc.cluster.local. udp 100 false 1232" NXDOMAIN qr,aa,rd 182 0.000092504s
[INFO] 10.244.0.21:46088 - 12504 "A IN kubernetes-services.default.svc.cluster.local.svc.cluster.local. udp 92 false 1232" NXDOMAIN qr,aa,rd 174 0.000115705s
[INFO] 10.244.0.21:46088 - 55636 "AAAA IN kubernetes-services.default.svc.cluster.local.svc.cluster.local. udp 92 false 1232" NXDOMAIN qr,aa,rd 174 0.00022931s
[INFO] 10.244.0.21:46088 - 31223 "AAAA IN kubernetes-services.default.svc.cluster.local.cluster.local. udp 88 false 1232" NXDOMAIN qr,aa,rd 170 0.000057803s
[INFO] 10.244.0.21:46088 - 22299 "A IN kubernetes-services.default.svc.cluster.local.cluster.local. udp 88 false 1232" NXDOMAIN qr,aa,rd 170 0.000091804s
[INFO] 10.244.0.21:46088 - 46381 "AAAA IN kubernetes-services.default.svc.cluster.local. udp 74 false 1232" NXDOMAIN qr,aa,rd 156 0.000112704s
[INFO] 10.244.0.21:46088 - 59259 "A IN kubernetes-services.default.svc.cluster.local. udp 74 false 1232" NXDOMAIN qr,aa,rd 156 0.000146607s
[INFO] 10.244.0.21:33890 - 55189 "AAAA IN kubernetes-service.default.svc.cluster.local.default.svc.cluster.local. udp 99 false 1232" NXDOMAIN qr,aa,rd 181 0.000146606s
[INFO] 10.244.0.21:33890 - 53256 "A IN kubernetes-service.default.svc.cluster.local.default.svc.cluster.local. udp 99 false 1232" NXDOMAIN qr,aa,rd 181 0.000116905s
[INFO] 10.244.0.21:33890 - 58724 "AAAA IN kubernetes-service.default.svc.cluster.local.svc.cluster.local. udp 91 false 1232" NXDOMAIN qr,aa,rd 173 0.000086804s
[INFO] 10.244.0.21:33890 - 36373 "A IN kubernetes-service.default.svc.cluster.local.svc.cluster.local. udp 91 false 1232" NXDOMAIN qr,aa,rd 173 0.000087404s
[INFO] 10.244.0.21:33890 - 28676 "A IN kubernetes-service.default.svc.cluster.local.cluster.local. udp 87 false 1232" NXDOMAIN qr,aa,rd 169 0.000091703s
[INFO] 10.244.0.21:33890 - 53943 "AAAA IN kubernetes-service.default.svc.cluster.local.cluster.local. udp 87 false 1232" NXDOMAIN qr,aa,rd 169 0.000131305s
[INFO] 10.244.0.21:33890 - 36823 "AAAA IN kubernetes-service.default.svc.cluster.local. udp 73 false 1232" NXDOMAIN qr,aa,rd 155 0.000183908s
[INFO] 10.244.0.21:33890 - 15186 "A IN kubernetes-service.default.svc.cluster.local. udp 73 false 1232" NXDOMAIN qr,aa,rd 155 0.000202309s
[INFO] 10.244.0.21:60455 - 21323 "AAAA IN curl.default.svc.cluster.local. udp 59 false 1232" NXDOMAIN qr,aa,rd 141 0.000210608s
[INFO] 10.244.0.21:60455 - 16421 "A IN curl.default.svc.cluster.local. udp 59 false 1232" NXDOMAIN qr,aa,rd 141 0.000284212s
[INFO] 10.244.0.21:60455 - 38058 "A IN curl.svc.cluster.local. udp 51 false 1232" NXDOMAIN qr,aa,rd 133 0.000234009s
[INFO] 10.244.0.21:60455 - 1908 "AAAA IN curl.svc.cluster.local. udp 51 false 1232" NXDOMAIN qr,aa,rd 133 0.000350014s
[INFO] 10.244.0.21:60455 - 34123 "AAAA IN curl.cluster.local. udp 47 false 1232" NXDOMAIN qr,aa,rd 129 0.000061103s
[INFO] 10.244.0.21:60455 - 10891 "A IN curl.cluster.local. udp 47 false 1232" NXDOMAIN qr,aa,rd 129 0.000071003s
[INFO] 10.244.0.21:52121 - 7726 "AAAA IN nginx-services.default.svc.cluster.local.default.svc.cluster.local. udp 95 false 1232" NXDOMAIN qr,aa,rd 177 0.000147909s
[INFO] 10.244.0.21:52121 - 30141 "A IN nginx-services.default.svc.cluster.local.default.svc.cluster.local. udp 95 false 1232" NXDOMAIN qr,aa,rd 177 0.000213713s
[INFO] 10.244.0.21:52121 - 64428 "AAAA IN nginx-services.default.svc.cluster.local.svc.cluster.local. udp 87 false 1232" NXDOMAIN qr,aa,rd 169 0.000087505s
[INFO] 10.244.0.21:52121 - 14465 "A IN nginx-services.default.svc.cluster.local.svc.cluster.local. udp 87 false 1232" NXDOMAIN qr,aa,rd 169 0.000084405s
[INFO] 10.244.0.21:52121 - 42069 "AAAA IN nginx-services.default.svc.cluster.local.cluster.local. udp 83 false 1232" NXDOMAIN qr,aa,rd 165 0.000095706s
[INFO] 10.244.0.21:52121 - 3172 "A IN nginx-services.default.svc.cluster.local.cluster.local. udp 83 false 1232" NXDOMAIN qr,aa,rd 165 0.000125107s
[INFO] 10.244.0.21:52121 - 8595 "AAAA IN nginx-services.default.svc.cluster.local. udp 69 false 1232" NOERROR qr,aa,rd 151 0.000104007s
[INFO] 10.244.0.21:52121 - 48124 "A IN nginx-services.default.svc.cluster.local. udp 69 false 1232" NOERROR qr,aa,rd 114 0.000145209s
[INFO] 10.244.0.21:60455 - 25834 "A IN curl. udp 33 false 1232" - - 0 6.004026259s
[ERROR] plugin/errors: 2 curl. A: read udp 10.244.0.2:54390->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.21:60455 - 24304 "AAAA IN curl. udp 33 false 1232" - - 0 6.005822873s
[ERROR] plugin/errors: 2 curl. AAAA: read udp 10.244.0.2:43458->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.22:52379 - 3709 "A IN example-service.default.svc.cluster.local.default.svc.cluster.local. udp 96 false 1232" NXDOMAIN qr,aa,rd 178 0.000493128s
[INFO] 10.244.0.22:52379 - 3929 "AAAA IN example-service.default.svc.cluster.local.default.svc.cluster.local. udp 96 false 1232" NXDOMAIN qr,aa,rd 178 0.000472927s
[INFO] 10.244.0.22:52379 - 29106 "AAAA IN example-service.default.svc.cluster.local.svc.cluster.local. udp 88 false 1232" NXDOMAIN qr,aa,rd 170 0.000091005s
[INFO] 10.244.0.22:52379 - 10619 "A IN example-service.default.svc.cluster.local.svc.cluster.local. udp 88 false 1232" NXDOMAIN qr,aa,rd 170 0.000117906s
[INFO] 10.244.0.22:52379 - 52386 "AAAA IN example-service.default.svc.cluster.local.cluster.local. udp 84 false 1232" NXDOMAIN qr,aa,rd 166 0.000098906s
[INFO] 10.244.0.22:52379 - 46106 "A IN example-service.default.svc.cluster.local.cluster.local. udp 84 false 1232" NXDOMAIN qr,aa,rd 166 0.000050203s
[INFO] 10.244.0.22:52379 - 14241 "A IN example.com. udp 40 false 1232" NOERROR qr,rd,ra 56 0.022704078s
[INFO] 10.244.0.22:52379 - 14241 "A IN example-service.default.svc.cluster.local. udp 70 false 1232" NOERROR qr,aa,rd 152 0.022885787s
[INFO] 10.244.0.22:52379 - 39896 "AAAA IN example.com. udp 40 false 1232" NOERROR qr,rd,ra 68 0.022820284s
[INFO] 10.244.0.22:52379 - 39896 "AAAA IN example-service.default.svc.cluster.local. udp 70 false 1232" NOERROR qr,aa,rd 164 0.023005194s
[INFO] 10.244.0.26:37096 - 39301 "A IN service-nginx.default.svc.cluster.local.default.svc.cluster.local. udp 94 false 1232" NXDOMAIN qr,aa,rd 176 0.000472327s
[INFO] 10.244.0.26:37096 - 30263 "AAAA IN service-nginx.default.svc.cluster.local.default.svc.cluster.local. udp 94 false 1232" NXDOMAIN qr,aa,rd 176 0.000536731s
[INFO] 10.244.0.26:37096 - 35876 "AAAA IN service-nginx.default.svc.cluster.local.svc.cluster.local. udp 86 false 1232" NXDOMAIN qr,aa,rd 168 0.00016721s
[INFO] 10.244.0.26:37096 - 53917 "A IN service-nginx.default.svc.cluster.local.svc.cluster.local. udp 86 false 1232" NXDOMAIN qr,aa,rd 168 0.000232013s
[INFO] 10.244.0.26:37096 - 14348 "AAAA IN service-nginx.default.svc.cluster.local.cluster.local. udp 82 false 1232" NXDOMAIN qr,aa,rd 164 0.000194011s
[INFO] 10.244.0.26:37096 - 50245 "A IN service-nginx.default.svc.cluster.local.cluster.local. udp 82 false 1232" NXDOMAIN qr,aa,rd 164 0.000246014s
[INFO] 10.244.0.26:37096 - 33899 "AAAA IN service-nginx.default.svc.cluster.local. udp 68 false 1232" NOERROR qr,aa,rd 150 0.000096406s
[INFO] 10.244.0.26:37096 - 55483 "A IN service-nginx.default.svc.cluster.local. udp 68 false 1232" NOERROR qr,aa,rd 112 0.000191911s


==> coredns [6d741091a3fd] <==
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:47087 - 52697 "HINFO IN 4385440397043783673.2769196845896072234. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.056106247s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    env=production
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=5883c09216182566a63dff4c326a6fc9ed2982ff
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_07_21T16_46_52_0700
                    minikube.k8s.io/version=v1.33.1
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 21 Jul 2024 09:46:45 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sun, 21 Jul 2024 13:48:16 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sun, 21 Jul 2024 13:45:52 +0000   Sun, 21 Jul 2024 09:46:44 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sun, 21 Jul 2024 13:45:52 +0000   Sun, 21 Jul 2024 09:46:44 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sun, 21 Jul 2024 13:45:52 +0000   Sun, 21 Jul 2024 09:46:44 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sun, 21 Jul 2024 13:45:52 +0000   Sun, 21 Jul 2024 09:46:45 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                6
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8108496Ki
  pods:               110
Allocatable:
  cpu:                6
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8108496Ki
  pods:               110
System Info:
  Machine ID:                 e75c0a4a437141b38c3d93dc46427528
  System UUID:                e75c0a4a437141b38c3d93dc46427528
  Boot ID:                    e69713a7-58ab-4729-8977-bad0b80793fd
  Kernel Version:             5.15.153.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://26.1.1
  Kubelet Version:            v1.30.0
  Kube-Proxy Version:         v1.30.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (10 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     nginx-rs-match-expression-9f7fr     0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4m51s
  default                     nginx-rs-match-expression-htrzc     0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4m51s
  default                     nginx-rs-match-expression-nlvcz     0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4m51s
  kube-system                 coredns-7db6d8ff4d-jwqcj            100m (1%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     4h1m
  kube-system                 etcd-minikube                       100m (1%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         4h1m
  kube-system                 kube-apiserver-minikube             250m (4%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4h1m
  kube-system                 kube-controller-manager-minikube    200m (3%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4h1m
  kube-system                 kube-proxy-xt2m7                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4h1m
  kube-system                 kube-scheduler-minikube             100m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4h1m
  kube-system                 storage-provisioner                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4h1m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (12%!)(MISSING)  0 (0%!)(MISSING)
  memory             170Mi (2%!)(MISSING)  170Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                    From             Message
  ----    ------                   ----                   ----             -------
  Normal  Starting                 7m25s                  kube-proxy       
  Normal  Starting                 7m38s                  kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  7m38s (x8 over 7m38s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    7m38s (x8 over 7m38s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     7m38s (x7 over 7m38s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  7m38s                  kubelet          Updated Node Allocatable limit across pods
  Normal  RegisteredNode           7m21s                  node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[  +0.411040] systemd-journald[40]: File /var/log/journal/54bce3ff141b419c96c46cd0bef53482/system.journal corrupted or uncleanly shut down, renaming and replacing.
[  +0.694471] /sbin/ldconfig: 
[  +0.000003] /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link

[  +0.022224] FS-Cache: Duplicate cookie detected
[  +0.000500] FS-Cache: O-cookie c=00000022 [p=00000002 fl=222 nc=0 na=1]
[  +0.000645] FS-Cache: O-cookie d=000000002d7e2d0b{9P.session} n=00000000ce00dd95
[  +0.000767] FS-Cache: O-key=[10] '34323934393337363838'
[  +0.000588] FS-Cache: N-cookie c=00000023 [p=00000002 fl=2 nc=0 na=1]
[  +0.000669] FS-Cache: N-cookie d=000000002d7e2d0b{9P.session} n=00000000ded5444f
[  +0.000550] FS-Cache: N-key=[10] '34323934393337363838'
[  +0.081343] WSL (1) ERROR: ConfigApplyWindowsLibPath:2537: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000003]  failed 2
[  +0.039109] FS-Cache: Duplicate cookie detected
[  +0.000494] FS-Cache: O-cookie c=00000026 [p=00000002 fl=222 nc=0 na=1]
[  +0.000445] FS-Cache: O-cookie d=000000002d7e2d0b{9P.session} n=00000000e8cb8796
[  +0.001826] FS-Cache: O-key=[10] '34323934393337373030'
[  +0.016163] FS-Cache: N-cookie c=00000027 [p=00000002 fl=2 nc=0 na=1]
[  +0.000699] FS-Cache: N-cookie d=000000002d7e2d0b{9P.session} n=000000005a8b25c5
[  +0.000511] FS-Cache: N-key=[10] '34323934393337373030'
[  +0.021292] WSL (1) WARNING: /usr/share/zoneinfo/Asia/Jakarta not found. Is the tzdata package installed?
[  +0.080779] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.023377] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.024731] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.002046] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001561] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.336220] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001206] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.172957] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000571] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000529] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000505] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000497] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000487] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000473] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000471] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000487] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000817] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000582] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000659] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000606] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000728] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.144250] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000792] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000594] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000532] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000638] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000583] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000547] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000552] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000530] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000534] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001816] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000811] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000758] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000550] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.299764] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.003882] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +5.428367] new mount options do not match the existing superblock, will be ignored
[  +0.000830] netlink: 'init': attribute type 4 has an invalid length.


==> etcd [3067d6a45f94] <==
{"level":"info","ts":"2024-07-21T12:30:17.982327Z","caller":"traceutil/trace.go:171","msg":"trace[1236312465] transaction","detail":"{read_only:false; response_revision:8982; number_of_response:1; }","duration":"308.560717ms","start":"2024-07-21T12:30:17.673757Z","end":"2024-07-21T12:30:17.982318Z","steps":["trace[1236312465] 'process raft request'  (duration: 105.267175ms)","trace[1236312465] 'compare'  (duration: 203.107334ms)"],"step_count":2}
{"level":"warn","ts":"2024-07-21T12:30:17.982362Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-21T12:30:17.673745Z","time spent":"308.598318ms","remote":"127.0.0.1:37092","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":673,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:8974 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:600 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >"}
{"level":"info","ts":"2024-07-21T12:31:44.112927Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":8811}
{"level":"info","ts":"2024-07-21T12:31:44.116511Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":8811,"took":"3.292426ms","hash":972942238,"current-db-size-bytes":1912832,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1126400,"current-db-size-in-use":"1.1 MB"}
{"level":"info","ts":"2024-07-21T12:31:44.116563Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":972942238,"revision":8811,"compact-revision":8571}
{"level":"info","ts":"2024-07-21T12:36:44.11023Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":9052}
{"level":"info","ts":"2024-07-21T12:36:44.113024Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":9052,"took":"2.14697ms","hash":674771921,"current-db-size-bytes":1912832,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1114112,"current-db-size-in-use":"1.1 MB"}
{"level":"info","ts":"2024-07-21T12:36:44.113095Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":674771921,"revision":9052,"compact-revision":8811}
{"level":"info","ts":"2024-07-21T12:41:44.114426Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":9291}
{"level":"info","ts":"2024-07-21T12:41:44.12019Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":9291,"took":"5.202626ms","hash":204375682,"current-db-size-bytes":1912832,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1114112,"current-db-size-in-use":"1.1 MB"}
{"level":"info","ts":"2024-07-21T12:41:44.120287Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":204375682,"revision":9291,"compact-revision":9052}
{"level":"info","ts":"2024-07-21T12:44:47.559534Z","caller":"traceutil/trace.go:171","msg":"trace[40056243] transaction","detail":"{read_only:false; response_revision:9679; number_of_response:1; }","duration":"157.811062ms","start":"2024-07-21T12:44:47.401336Z","end":"2024-07-21T12:44:47.559147Z","steps":["trace[40056243] 'process raft request'  (duration: 157.672056ms)"],"step_count":1}
{"level":"info","ts":"2024-07-21T12:44:59.552507Z","caller":"traceutil/trace.go:171","msg":"trace[1387072823] transaction","detail":"{read_only:false; response_revision:9689; number_of_response:1; }","duration":"175.776593ms","start":"2024-07-21T12:44:59.376713Z","end":"2024-07-21T12:44:59.55249Z","steps":["trace[1387072823] 'process raft request'  (duration: 87.930798ms)","trace[1387072823] 'compare'  (duration: 87.600484ms)"],"step_count":2}
{"level":"warn","ts":"2024-07-21T12:45:01.773796Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"119.701434ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-07-21T12:45:01.774251Z","caller":"traceutil/trace.go:171","msg":"trace[664418484] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:9691; }","duration":"120.252057ms","start":"2024-07-21T12:45:01.653955Z","end":"2024-07-21T12:45:01.774207Z","steps":["trace[664418484] 'range keys from in-memory index tree'  (duration: 119.692633ms)"],"step_count":1}
{"level":"info","ts":"2024-07-21T12:45:05.781674Z","caller":"traceutil/trace.go:171","msg":"trace[1838591275] transaction","detail":"{read_only:false; response_revision:9693; number_of_response:1; }","duration":"105.488651ms","start":"2024-07-21T12:45:05.676173Z","end":"2024-07-21T12:45:05.781661Z","steps":["trace[1838591275] 'process raft request'  (duration: 105.401843ms)"],"step_count":1}
{"level":"warn","ts":"2024-07-21T12:45:06.658715Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"274.62169ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128030670652385189 > lease_revoke:<id:70cc90d4afb7275e>","response":"size:29"}
{"level":"info","ts":"2024-07-21T12:45:07.94281Z","caller":"traceutil/trace.go:171","msg":"trace[636703652] transaction","detail":"{read_only:false; response_revision:9694; number_of_response:1; }","duration":"155.529852ms","start":"2024-07-21T12:45:07.787241Z","end":"2024-07-21T12:45:07.942771Z","steps":["trace[636703652] 'process raft request'  (duration: 155.167615ms)"],"step_count":1}
{"level":"info","ts":"2024-07-21T12:45:08.317612Z","caller":"traceutil/trace.go:171","msg":"trace[572066250] linearizableReadLoop","detail":"{readStateIndex:11920; appliedIndex:11919; }","duration":"215.014715ms","start":"2024-07-21T12:45:08.102583Z","end":"2024-07-21T12:45:08.317598Z","steps":["trace[572066250] 'read index received'  (duration: 169.24565ms)","trace[572066250] 'applied index is now lower than readState.Index'  (duration: 45.768565ms)"],"step_count":2}
{"level":"warn","ts":"2024-07-21T12:45:08.317719Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"215.076921ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/resourcequotas/\" range_end:\"/registry/resourcequotas0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-07-21T12:45:08.317681Z","caller":"traceutil/trace.go:171","msg":"trace[878207332] transaction","detail":"{read_only:false; response_revision:9695; number_of_response:1; }","duration":"240.934756ms","start":"2024-07-21T12:45:08.076727Z","end":"2024-07-21T12:45:08.317662Z","steps":["trace[878207332] 'process raft request'  (duration: 195.141289ms)","trace[878207332] 'compare'  (duration: 45.579446ms)"],"step_count":2}
{"level":"info","ts":"2024-07-21T12:45:08.317738Z","caller":"traceutil/trace.go:171","msg":"trace[1220851970] range","detail":"{range_begin:/registry/resourcequotas/; range_end:/registry/resourcequotas0; response_count:0; response_revision:9695; }","duration":"215.169731ms","start":"2024-07-21T12:45:08.102563Z","end":"2024-07-21T12:45:08.317733Z","steps":["trace[1220851970] 'agreement among raft nodes before linearized reading'  (duration: 215.079621ms)"],"step_count":1}
{"level":"info","ts":"2024-07-21T12:46:44.107289Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":9531}
{"level":"info","ts":"2024-07-21T12:46:44.110101Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":9531,"took":"2.547384ms","hash":896501531,"current-db-size-bytes":1912832,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1118208,"current-db-size-in-use":"1.1 MB"}
{"level":"info","ts":"2024-07-21T12:46:44.110161Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":896501531,"revision":9531,"compact-revision":9291}
{"level":"info","ts":"2024-07-21T12:51:44.098637Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":9773}
{"level":"info","ts":"2024-07-21T12:51:44.102426Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":9773,"took":"3.433752ms","hash":2168600980,"current-db-size-bytes":2048000,"current-db-size":"2.0 MB","current-db-size-in-use-bytes":1413120,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-07-21T12:51:44.102489Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2168600980,"revision":9773,"compact-revision":9531}
{"level":"info","ts":"2024-07-21T12:56:44.089815Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":10080}
{"level":"info","ts":"2024-07-21T12:56:44.093075Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":10080,"took":"3.05806ms","hash":523301759,"current-db-size-bytes":2048000,"current-db-size":"2.0 MB","current-db-size-in-use-bytes":1425408,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-07-21T12:56:44.09312Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":523301759,"revision":10080,"compact-revision":9773}
{"level":"info","ts":"2024-07-21T13:01:44.084366Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":10321}
{"level":"info","ts":"2024-07-21T13:01:44.088899Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":10321,"took":"4.269316ms","hash":3978190327,"current-db-size-bytes":2048000,"current-db-size":"2.0 MB","current-db-size-in-use-bytes":970752,"current-db-size-in-use":"971 kB"}
{"level":"info","ts":"2024-07-21T13:01:44.088963Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3978190327,"revision":10321,"compact-revision":10080}
{"level":"info","ts":"2024-07-21T13:06:44.079679Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":10562}
{"level":"info","ts":"2024-07-21T13:06:44.086151Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":10562,"took":"6.067605ms","hash":2407020520,"current-db-size-bytes":2048000,"current-db-size":"2.0 MB","current-db-size-in-use-bytes":991232,"current-db-size-in-use":"991 kB"}
{"level":"info","ts":"2024-07-21T13:06:44.086257Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2407020520,"revision":10562,"compact-revision":10321}
{"level":"info","ts":"2024-07-21T13:09:46.196308Z","caller":"traceutil/trace.go:171","msg":"trace[1082371839] transaction","detail":"{read_only:false; response_revision:10947; number_of_response:1; }","duration":"100.516965ms","start":"2024-07-21T13:09:46.095663Z","end":"2024-07-21T13:09:46.19618Z","steps":["trace[1082371839] 'process raft request'  (duration: 100.390558ms)"],"step_count":1}
{"level":"warn","ts":"2024-07-21T13:09:46.944751Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"285.796715ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128030670652391082 > lease_revoke:<id:70cc90d4afb73e70>","response":"size:29"}
{"level":"info","ts":"2024-07-21T13:09:51.454636Z","caller":"traceutil/trace.go:171","msg":"trace[1892248227] linearizableReadLoop","detail":"{readStateIndex:13486; appliedIndex:13485; }","duration":"200.4519ms","start":"2024-07-21T13:09:51.254171Z","end":"2024-07-21T13:09:51.454623Z","steps":["trace[1892248227] 'read index received'  (duration: 200.354295ms)","trace[1892248227] 'applied index is now lower than readState.Index'  (duration: 97.105¬µs)"],"step_count":2}
{"level":"warn","ts":"2024-07-21T13:09:51.454788Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"200.546004ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-07-21T13:09:51.45481Z","caller":"traceutil/trace.go:171","msg":"trace[818953897] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:10952; }","duration":"200.67131ms","start":"2024-07-21T13:09:51.254134Z","end":"2024-07-21T13:09:51.454805Z","steps":["trace[818953897] 'agreement among raft nodes before linearized reading'  (duration: 200.553704ms)"],"step_count":1}
{"level":"info","ts":"2024-07-21T13:09:51.455511Z","caller":"traceutil/trace.go:171","msg":"trace[771346524] transaction","detail":"{read_only:false; response_revision:10952; number_of_response:1; }","duration":"284.262239ms","start":"2024-07-21T13:09:51.170479Z","end":"2024-07-21T13:09:51.454741Z","steps":["trace[771346524] 'process raft request'  (duration: 284.07563ms)"],"step_count":1}
{"level":"warn","ts":"2024-07-21T13:09:51.831354Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"253.966143ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-07-21T13:09:51.831511Z","caller":"traceutil/trace.go:171","msg":"trace[827325241] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:10952; }","duration":"254.131151ms","start":"2024-07-21T13:09:51.57737Z","end":"2024-07-21T13:09:51.831501Z","steps":["trace[827325241] 'range keys from in-memory index tree'  (duration: 253.949542ms)"],"step_count":1}
{"level":"info","ts":"2024-07-21T13:11:44.07399Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":10802}
{"level":"info","ts":"2024-07-21T13:11:44.077344Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":10802,"took":"2.965446ms","hash":3405414622,"current-db-size-bytes":2048000,"current-db-size":"2.0 MB","current-db-size-in-use-bytes":937984,"current-db-size-in-use":"938 kB"}
{"level":"info","ts":"2024-07-21T13:11:44.077396Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3405414622,"revision":10802,"compact-revision":10562}
{"level":"info","ts":"2024-07-21T13:14:05.565383Z","caller":"traceutil/trace.go:171","msg":"trace[965058683] transaction","detail":"{read_only:false; response_revision:11157; number_of_response:1; }","duration":"151.518583ms","start":"2024-07-21T13:14:05.413848Z","end":"2024-07-21T13:14:05.565367Z","steps":["trace[965058683] 'process raft request'  (duration: 151.371175ms)"],"step_count":1}
{"level":"info","ts":"2024-07-21T13:16:44.074841Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":11043}
{"level":"info","ts":"2024-07-21T13:16:44.080402Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":11043,"took":"5.07104ms","hash":181389039,"current-db-size-bytes":2048000,"current-db-size":"2.0 MB","current-db-size-in-use-bytes":954368,"current-db-size-in-use":"954 kB"}
{"level":"info","ts":"2024-07-21T13:16:44.080507Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":181389039,"revision":11043,"compact-revision":10802}
{"level":"warn","ts":"2024-07-21T13:17:18.519487Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"160.860875ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:601"}
{"level":"info","ts":"2024-07-21T13:17:18.519551Z","caller":"traceutil/trace.go:171","msg":"trace[417632737] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:11310; }","duration":"160.998481ms","start":"2024-07-21T13:17:18.358545Z","end":"2024-07-21T13:17:18.519544Z","steps":["trace[417632737] 'agreement among raft nodes before linearized reading'  (duration: 160.852976ms)"],"step_count":1}
{"level":"info","ts":"2024-07-21T13:17:18.519243Z","caller":"traceutil/trace.go:171","msg":"trace[295061798] linearizableReadLoop","detail":"{readStateIndex:13935; appliedIndex:13934; }","duration":"160.642468ms","start":"2024-07-21T13:17:18.358583Z","end":"2024-07-21T13:17:18.519225Z","steps":["trace[295061798] 'read index received'  (duration: 139.821682ms)","trace[295061798] 'applied index is now lower than readState.Index'  (duration: 20.820386ms)"],"step_count":2}
{"level":"info","ts":"2024-07-21T13:17:18.519305Z","caller":"traceutil/trace.go:171","msg":"trace[1437246400] transaction","detail":"{read_only:false; response_revision:11310; number_of_response:1; }","duration":"189.815769ms","start":"2024-07-21T13:17:18.329473Z","end":"2024-07-21T13:17:18.519289Z","steps":["trace[1437246400] 'process raft request'  (duration: 168.91008ms)","trace[1437246400] 'get key's previous created_revision and leaseID' {req_type:put; key:/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii; req_size:670; } (duration: 20.742383ms)"],"step_count":2}
{"level":"info","ts":"2024-07-21T13:17:21.371552Z","caller":"traceutil/trace.go:171","msg":"trace[999528075] linearizableReadLoop","detail":"{readStateIndex:13939; appliedIndex:13938; }","duration":"145.467394ms","start":"2024-07-21T13:17:21.226073Z","end":"2024-07-21T13:17:21.37154Z","steps":["trace[999528075] 'read index received'  (duration: 56.307527ms)","trace[999528075] 'applied index is now lower than readState.Index'  (duration: 89.145267ms)"],"step_count":2}
{"level":"info","ts":"2024-07-21T13:17:21.371694Z","caller":"traceutil/trace.go:171","msg":"trace[65342941] transaction","detail":"{read_only:false; response_revision:11313; number_of_response:1; }","duration":"178.218932ms","start":"2024-07-21T13:17:21.193463Z","end":"2024-07-21T13:17:21.371682Z","steps":["trace[65342941] 'process raft request'  (duration: 89.038863ms)","trace[65342941] 'compare'  (duration: 88.875257ms)"],"step_count":2}
{"level":"warn","ts":"2024-07-21T13:17:21.37176Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"145.678002ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-07-21T13:17:21.371825Z","caller":"traceutil/trace.go:171","msg":"trace[1414605280] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:11313; }","duration":"145.765106ms","start":"2024-07-21T13:17:21.226054Z","end":"2024-07-21T13:17:21.371819Z","steps":["trace[1414605280] 'agreement among raft nodes before linearized reading'  (duration: 145.656102ms)"],"step_count":1}


==> etcd [fa8dfc74eb53] <==
{"level":"info","ts":"2024-07-21T13:40:44.799279Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2024-07-21T13:40:44.799294Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 3"}
{"level":"info","ts":"2024-07-21T13:40:44.7993Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 3"}
{"level":"info","ts":"2024-07-21T13:40:44.799307Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 3"}
{"level":"info","ts":"2024-07-21T13:40:44.799317Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 3"}
{"level":"info","ts":"2024-07-21T13:40:44.801588Z","caller":"etcdserver/server.go:2068","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-07-21T13:40:44.80175Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-07-21T13:40:44.802158Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-07-21T13:40:44.802329Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-07-21T13:40:44.802357Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-07-21T13:40:44.803443Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-07-21T13:40:44.803462Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-07-21T13:40:47.000878Z","caller":"traceutil/trace.go:171","msg":"trace[669530843] transaction","detail":"{read_only:false; number_of_response:1; response_revision:11401; }","duration":"251.299055ms","start":"2024-07-21T13:40:46.749523Z","end":"2024-07-21T13:40:47.000822Z","steps":["trace[669530843] 'process raft request'  (duration: 251.134044ms)"],"step_count":1}
{"level":"info","ts":"2024-07-21T13:40:47.10173Z","caller":"traceutil/trace.go:171","msg":"trace[702436593] linearizableReadLoop","detail":"{readStateIndex:14052; appliedIndex:14049; }","duration":"258.890283ms","start":"2024-07-21T13:40:46.842829Z","end":"2024-07-21T13:40:47.101719Z","steps":["trace[702436593] 'read index received'  (duration: 157.875066ms)","trace[702436593] 'applied index is now lower than readState.Index'  (duration: 101.014917ms)"],"step_count":2}
{"level":"info","ts":"2024-07-21T13:40:47.101801Z","caller":"traceutil/trace.go:171","msg":"trace[877469098] transaction","detail":"{read_only:false; response_revision:11402; number_of_response:1; }","duration":"296.377887ms","start":"2024-07-21T13:40:46.805417Z","end":"2024-07-21T13:40:47.101795Z","steps":["trace[877469098] 'process raft request'  (duration: 296.261879ms)"],"step_count":1}
{"level":"info","ts":"2024-07-21T13:40:47.101871Z","caller":"traceutil/trace.go:171","msg":"trace[1074579191] transaction","detail":"{read_only:false; number_of_response:0; response_revision:11401; }","duration":"348.788627ms","start":"2024-07-21T13:40:46.753079Z","end":"2024-07-21T13:40:47.101868Z","steps":["trace[1074579191] 'process raft request'  (duration: 336.757591ms)"],"step_count":1}
{"level":"warn","ts":"2024-07-21T13:40:47.102017Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"196.210629ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/coredns\" ","response":"range_response_count:1 size:179"}
{"level":"info","ts":"2024-07-21T13:40:47.102043Z","caller":"traceutil/trace.go:171","msg":"trace[1255563625] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/coredns; range_end:; response_count:1; response_revision:11402; }","duration":"196.247732ms","start":"2024-07-21T13:40:46.905789Z","end":"2024-07-21T13:40:47.102037Z","steps":["trace[1255563625] 'agreement among raft nodes before linearized reading'  (duration: 196.203829ms)"],"step_count":1}
{"level":"warn","ts":"2024-07-21T13:40:47.102019Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"259.199105ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/kube-system/storage-provisioner\" ","response":"range_response_count:1 size:4132"}
{"level":"info","ts":"2024-07-21T13:40:47.102085Z","caller":"traceutil/trace.go:171","msg":"trace[1729567130] range","detail":"{range_begin:/registry/pods/kube-system/storage-provisioner; range_end:; response_count:1; response_revision:11402; }","duration":"259.291711ms","start":"2024-07-21T13:40:46.842789Z","end":"2024-07-21T13:40:47.102081Z","steps":["trace[1729567130] 'agreement among raft nodes before linearized reading'  (duration: 259.177803ms)"],"step_count":1}
{"level":"warn","ts":"2024-07-21T13:40:47.102103Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-21T13:40:46.753065Z","time spent":"348.821529ms","remote":"127.0.0.1:43658","response type":"/etcdserverpb.KV/Txn","request count":0,"request size":0,"response count":0,"response size":29,"request content":"compare:<target:MOD key:\"/registry/minions/minikube\" mod_revision:0 > success:<request_put:<key:\"/registry/minions/minikube\" value_size:3205 >> failure:<>"}
{"level":"warn","ts":"2024-07-21T13:40:47.102254Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"196.482248ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/kube-proxy\" ","response":"range_response_count:1 size:185"}
{"level":"info","ts":"2024-07-21T13:40:47.102268Z","caller":"traceutil/trace.go:171","msg":"trace[1175053428] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/kube-proxy; range_end:; response_count:1; response_revision:11402; }","duration":"196.51135ms","start":"2024-07-21T13:40:46.905752Z","end":"2024-07-21T13:40:47.102264Z","steps":["trace[1175053428] 'agreement among raft nodes before linearized reading'  (duration: 196.483648ms)"],"step_count":1}
{"level":"warn","ts":"2024-07-21T13:40:47.102319Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"202.949298ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/storageclasses/standard\" ","response":"range_response_count:1 size:992"}
{"level":"info","ts":"2024-07-21T13:40:47.102341Z","caller":"traceutil/trace.go:171","msg":"trace[1388947964] range","detail":"{range_begin:/registry/storageclasses/standard; range_end:; response_count:1; response_revision:11402; }","duration":"203.396429ms","start":"2024-07-21T13:40:46.898938Z","end":"2024-07-21T13:40:47.102334Z","steps":["trace[1388947964] 'agreement among raft nodes before linearized reading'  (duration: 202.899994ms)"],"step_count":1}
{"level":"warn","ts":"2024-07-21T13:40:47.102348Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"196.548052ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/storage-provisioner\" ","response":"range_response_count:1 size:721"}
{"level":"info","ts":"2024-07-21T13:40:47.102384Z","caller":"traceutil/trace.go:171","msg":"trace[1783937745] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/storage-provisioner; range_end:; response_count:1; response_revision:11402; }","duration":"196.580255ms","start":"2024-07-21T13:40:46.905785Z","end":"2024-07-21T13:40:47.102365Z","steps":["trace[1783937745] 'agreement among raft nodes before linearized reading'  (duration: 196.540952ms)"],"step_count":1}
{"level":"info","ts":"2024-07-21T13:40:47.635077Z","caller":"traceutil/trace.go:171","msg":"trace[789662822] linearizableReadLoop","detail":"{readStateIndex:14058; appliedIndex:14057; }","duration":"264.60148ms","start":"2024-07-21T13:40:47.370462Z","end":"2024-07-21T13:40:47.635064Z","steps":["trace[789662822] 'read index received'  (duration: 226.061903ms)","trace[789662822] 'applied index is now lower than readState.Index'  (duration: 38.539077ms)"],"step_count":2}
{"level":"warn","ts":"2024-07-21T13:40:47.635176Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"264.703287ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:601"}
{"level":"info","ts":"2024-07-21T13:40:47.635197Z","caller":"traceutil/trace.go:171","msg":"trace[1393818322] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:11407; }","duration":"264.74799ms","start":"2024-07-21T13:40:47.370443Z","end":"2024-07-21T13:40:47.635191Z","steps":["trace[1393818322] 'agreement among raft nodes before linearized reading'  (duration: 264.703887ms)"],"step_count":1}
{"level":"info","ts":"2024-07-21T13:40:47.635173Z","caller":"traceutil/trace.go:171","msg":"trace[1791317912] transaction","detail":"{read_only:false; response_revision:11407; number_of_response:1; }","duration":"271.07383ms","start":"2024-07-21T13:40:47.364088Z","end":"2024-07-21T13:40:47.635162Z","steps":["trace[1791317912] 'process raft request'  (duration: 232.443946ms)","trace[1791317912] 'compare'  (duration: 38.459472ms)"],"step_count":2}
{"level":"info","ts":"2024-07-21T13:40:48.006592Z","caller":"traceutil/trace.go:171","msg":"trace[1277513006] linearizableReadLoop","detail":"{readStateIndex:14062; appliedIndex:14061; }","duration":"198.206867ms","start":"2024-07-21T13:40:47.80835Z","end":"2024-07-21T13:40:48.006557Z","steps":["trace[1277513006] 'read index received'  (duration: 102.581225ms)","trace[1277513006] 'applied index is now lower than readState.Index'  (duration: 95.623742ms)"],"step_count":2}
{"level":"info","ts":"2024-07-21T13:40:48.006841Z","caller":"traceutil/trace.go:171","msg":"trace[138360935] transaction","detail":"{read_only:false; response_revision:11411; number_of_response:1; }","duration":"199.180735ms","start":"2024-07-21T13:40:47.807608Z","end":"2024-07-21T13:40:48.006789Z","steps":["trace[138360935] 'process raft request'  (duration: 103.375881ms)","trace[138360935] 'compare'  (duration: 95.408127ms)"],"step_count":2}
{"level":"warn","ts":"2024-07-21T13:40:48.006878Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"198.494688ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterroles/admin\" ","response":"range_response_count:1 size:3548"}
{"level":"info","ts":"2024-07-21T13:40:48.006957Z","caller":"traceutil/trace.go:171","msg":"trace[1763259035] range","detail":"{range_begin:/registry/clusterroles/admin; range_end:; response_count:1; response_revision:11411; }","duration":"198.605896ms","start":"2024-07-21T13:40:47.808331Z","end":"2024-07-21T13:40:48.006936Z","steps":["trace[1763259035] 'agreement among raft nodes before linearized reading'  (duration: 198.447285ms)"],"step_count":1}
{"level":"warn","ts":"2024-07-21T13:40:48.209945Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"102.881246ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterroles/system:controller:ttl-controller\" ","response":"range_response_count:1 size:663"}
{"level":"info","ts":"2024-07-21T13:40:48.210001Z","caller":"traceutil/trace.go:171","msg":"trace[762725542] range","detail":"{range_begin:/registry/clusterroles/system:controller:ttl-controller; range_end:; response_count:1; response_revision:11411; }","duration":"102.962952ms","start":"2024-07-21T13:40:48.107028Z","end":"2024-07-21T13:40:48.209991Z","steps":["trace[762725542] 'range keys from in-memory index tree'  (duration: 102.79954ms)"],"step_count":1}
{"level":"info","ts":"2024-07-21T13:40:51.411266Z","caller":"traceutil/trace.go:171","msg":"trace[2132706659] transaction","detail":"{read_only:false; response_revision:11413; number_of_response:1; }","duration":"105.447525ms","start":"2024-07-21T13:40:51.305806Z","end":"2024-07-21T13:40:51.411254Z","steps":["trace[2132706659] 'process raft request'  (duration: 105.346618ms)"],"step_count":1}
{"level":"info","ts":"2024-07-21T13:40:52.405151Z","caller":"traceutil/trace.go:171","msg":"trace[2057520861] transaction","detail":"{read_only:false; response_revision:11417; number_of_response:1; }","duration":"104.972691ms","start":"2024-07-21T13:40:52.300161Z","end":"2024-07-21T13:40:52.405134Z","steps":["trace[2057520861] 'process raft request'  (duration: 80.382883ms)","trace[2057520861] 'compare'  (duration: 24.4755ms)"],"step_count":2}
{"level":"info","ts":"2024-07-21T13:40:52.627929Z","caller":"traceutil/trace.go:171","msg":"trace[731924095] transaction","detail":"{read_only:false; response_revision:11420; number_of_response:1; }","duration":"132.210583ms","start":"2024-07-21T13:40:52.495705Z","end":"2024-07-21T13:40:52.627916Z","steps":["trace[731924095] 'process raft request'  (duration: 59.945863ms)","trace[731924095] 'compare'  (duration: 72.176414ms)"],"step_count":2}
{"level":"info","ts":"2024-07-21T13:40:52.645442Z","caller":"traceutil/trace.go:171","msg":"trace[686859350] transaction","detail":"{read_only:false; response_revision:11421; number_of_response:1; }","duration":"145.787227ms","start":"2024-07-21T13:40:52.499643Z","end":"2024-07-21T13:40:52.64543Z","steps":["trace[686859350] 'process raft request'  (duration: 145.596213ms)"],"step_count":1}
{"level":"info","ts":"2024-07-21T13:40:52.829007Z","caller":"traceutil/trace.go:171","msg":"trace[604495468] transaction","detail":"{read_only:false; response_revision:11424; number_of_response:1; }","duration":"107.117615ms","start":"2024-07-21T13:40:52.721875Z","end":"2024-07-21T13:40:52.828992Z","steps":["trace[604495468] 'process raft request'  (duration: 51.383231ms)","trace[604495468] 'compare'  (duration: 55.651793ms)"],"step_count":2}
{"level":"info","ts":"2024-07-21T13:40:53.019387Z","caller":"traceutil/trace.go:171","msg":"trace[461379222] transaction","detail":"{read_only:false; response_revision:11426; number_of_response:1; }","duration":"148.2249ms","start":"2024-07-21T13:40:52.871148Z","end":"2024-07-21T13:40:53.019373Z","steps":["trace[461379222] 'process raft request'  (duration: 97.299623ms)","trace[461379222] 'compare'  (duration: 50.869383ms)"],"step_count":2}
{"level":"warn","ts":"2024-07-21T13:40:53.304451Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"174.000056ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128030674246758264 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/events/default/minikube.17e43e1ae5e49e94\" mod_revision:11424 > success:<request_put:<key:\"/registry/events/default/minikube.17e43e1ae5e49e94\" value_size:574 lease:8128030674246758240 >> failure:<request_range:<key:\"/registry/events/default/minikube.17e43e1ae5e49e94\" > >>","response":"size:16"}
{"level":"info","ts":"2024-07-21T13:40:53.304534Z","caller":"traceutil/trace.go:171","msg":"trace[68774873] transaction","detail":"{read_only:false; response_revision:11427; number_of_response:1; }","duration":"281.764506ms","start":"2024-07-21T13:40:53.022759Z","end":"2024-07-21T13:40:53.304523Z","steps":["trace[68774873] 'process raft request'  (duration: 107.519774ms)","trace[68774873] 'compare'  (duration: 173.927564ms)"],"step_count":2}
{"level":"info","ts":"2024-07-21T13:40:53.544193Z","caller":"traceutil/trace.go:171","msg":"trace[1673551027] transaction","detail":"{read_only:false; response_revision:11430; number_of_response:1; }","duration":"161.534135ms","start":"2024-07-21T13:40:53.382646Z","end":"2024-07-21T13:40:53.54418Z","steps":["trace[1673551027] 'process raft request'  (duration: 146.229304ms)","trace[1673551027] 'compare'  (duration: 15.246037ms)"],"step_count":2}
{"level":"info","ts":"2024-07-21T13:40:53.750923Z","caller":"traceutil/trace.go:171","msg":"trace[1742827224] transaction","detail":"{read_only:false; response_revision:11432; number_of_response:1; }","duration":"157.353164ms","start":"2024-07-21T13:40:53.59355Z","end":"2024-07-21T13:40:53.750903Z","steps":["trace[1742827224] 'process raft request'  (duration: 97.064546ms)","trace[1742827224] 'compare'  (duration: 60.213825ms)"],"step_count":2}
{"level":"info","ts":"2024-07-21T13:40:53.941982Z","caller":"traceutil/trace.go:171","msg":"trace[1351111545] transaction","detail":"{read_only:false; response_revision:11434; number_of_response:1; }","duration":"116.638439ms","start":"2024-07-21T13:40:53.825329Z","end":"2024-07-21T13:40:53.941967Z","steps":["trace[1351111545] 'process raft request'  (duration: 54.257536ms)","trace[1351111545] 'compare'  (duration: 62.248316ms)"],"step_count":2}
{"level":"info","ts":"2024-07-21T13:40:54.14288Z","caller":"traceutil/trace.go:171","msg":"trace[1366781256] transaction","detail":"{read_only:false; response_revision:11435; number_of_response:1; }","duration":"197.395057ms","start":"2024-07-21T13:40:53.94547Z","end":"2024-07-21T13:40:54.142865Z","steps":["trace[1366781256] 'process raft request'  (duration: 98.938954ms)","trace[1366781256] 'compare'  (duration: 98.360713ms)"],"step_count":2}
{"level":"warn","ts":"2024-07-21T13:45:08.431657Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"120.295359ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/192.168.49.2\" ","response":"range_response_count:1 size:131"}
{"level":"info","ts":"2024-07-21T13:45:08.431725Z","caller":"traceutil/trace.go:171","msg":"trace[1162519807] range","detail":"{range_begin:/registry/masterleases/192.168.49.2; range_end:; response_count:1; response_revision:11727; }","duration":"120.386263ms","start":"2024-07-21T13:45:08.311327Z","end":"2024-07-21T13:45:08.431713Z","steps":["trace[1162519807] 'range keys from in-memory index tree'  (duration: 120.240556ms)"],"step_count":1}
{"level":"info","ts":"2024-07-21T13:45:08.65188Z","caller":"traceutil/trace.go:171","msg":"trace[2135760039] transaction","detail":"{read_only:false; response_revision:11728; number_of_response:1; }","duration":"132.716694ms","start":"2024-07-21T13:45:08.519148Z","end":"2024-07-21T13:45:08.651865Z","steps":["trace[2135760039] 'process raft request'  (duration: 43.081205ms)","trace[2135760039] 'compare'  (duration: 89.489282ms)"],"step_count":2}
{"level":"info","ts":"2024-07-21T13:45:10.36724Z","caller":"traceutil/trace.go:171","msg":"trace[934687951] transaction","detail":"{read_only:false; response_revision:11729; number_of_response:1; }","duration":"118.164221ms","start":"2024-07-21T13:45:10.249062Z","end":"2024-07-21T13:45:10.367226Z","steps":["trace[934687951] 'process raft request'  (duration: 118.046914ms)"],"step_count":1}
{"level":"info","ts":"2024-07-21T13:45:12.596367Z","caller":"traceutil/trace.go:171","msg":"trace[963183747] transaction","detail":"{read_only:false; response_revision:11732; number_of_response:1; }","duration":"143.879102ms","start":"2024-07-21T13:45:12.452473Z","end":"2024-07-21T13:45:12.596353Z","steps":["trace[963183747] 'process raft request'  (duration: 47.947533ms)","trace[963183747] 'compare'  (duration: 95.878265ms)"],"step_count":2}
{"level":"warn","ts":"2024-07-21T13:45:13.194689Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"259.872181ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-07-21T13:45:13.194732Z","caller":"traceutil/trace.go:171","msg":"trace[1136737576] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:11732; }","duration":"259.940986ms","start":"2024-07-21T13:45:12.934782Z","end":"2024-07-21T13:45:13.194723Z","steps":["trace[1136737576] 'range keys from in-memory index tree'  (duration: 259.831879ms)"],"step_count":1}
{"level":"info","ts":"2024-07-21T13:45:13.89505Z","caller":"traceutil/trace.go:171","msg":"trace[2082011721] linearizableReadLoop","detail":"{readStateIndex:14441; appliedIndex:14440; }","duration":"126.40826ms","start":"2024-07-21T13:45:13.768629Z","end":"2024-07-21T13:45:13.895037Z","steps":["trace[2082011721] 'read index received'  (duration: 28.020031ms)","trace[2082011721] 'applied index is now lower than readState.Index'  (duration: 98.387429ms)"],"step_count":2}
{"level":"warn","ts":"2024-07-21T13:45:13.895164Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"126.519367ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/\" range_end:\"/registry/events0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2024-07-21T13:45:13.895191Z","caller":"traceutil/trace.go:171","msg":"trace[435125616] range","detail":"{range_begin:/registry/events/; range_end:/registry/events0; response_count:0; response_revision:11732; }","duration":"126.575471ms","start":"2024-07-21T13:45:13.768609Z","end":"2024-07-21T13:45:13.895184Z","steps":["trace[435125616] 'agreement among raft nodes before linearized reading'  (duration: 126.494966ms)"],"step_count":1}
{"level":"info","ts":"2024-07-21T13:45:14.530395Z","caller":"traceutil/trace.go:171","msg":"trace[769814456] transaction","detail":"{read_only:false; response_revision:11733; number_of_response:1; }","duration":"102.473796ms","start":"2024-07-21T13:45:14.427908Z","end":"2024-07-21T13:45:14.530381Z","steps":["trace[769814456] 'process raft request'  (duration: 102.384891ms)"],"step_count":1}


==> kernel <==
 13:48:20 up 8 min,  0 users,  load average: 0.31, 0.53, 0.39
Linux minikube 5.15.153.1-microsoft-standard-WSL2 #1 SMP Fri Mar 29 23:14:13 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [92545170e0c2] <==
I0721 09:46:50.942362       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0721 09:46:51.618323       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0721 09:46:51.938787       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0721 09:46:51.949440       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0721 09:46:51.957681       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0721 09:47:05.517983       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0721 09:47:05.767907       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
I0721 09:58:09.181710       1 controller.go:615] quota admission added evaluator for: jobs.batch
I0721 10:38:43.078900       1 controller.go:615] quota admission added evaluator for: cronjobs.batch
I0721 11:12:04.481549       1 trace.go:236] Trace[579758762]: "Update" accept:application/json, */*,audit-id:18a28fd5-1e38-4894-a3ed-67cf08d6b23e,client:192.168.49.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (21-Jul-2024 11:12:03.879) (total time: 601ms):
Trace[579758762]: ["GuaranteedUpdate etcd3" audit-id:18a28fd5-1e38-4894-a3ed-67cf08d6b23e,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 601ms (11:12:03.879)
Trace[579758762]:  ---"Txn call completed" 601ms (11:12:04.480)]
Trace[579758762]: [601.748033ms] [601.748033ms] END
I0721 11:12:05.848143       1 trace.go:236] Trace[1808226458]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:19c2b7bc-1279-474d-9502-90ea6aa54612,client:192.168.49.2,api-group:coordination.k8s.io,api-version:v1,name:minikube,subresource:,namespace:kube-node-lease,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:PUT (21-Jul-2024 11:12:05.012) (total time: 835ms):
Trace[1808226458]: ["GuaranteedUpdate etcd3" audit-id:19c2b7bc-1279-474d-9502-90ea6aa54612,key:/leases/kube-node-lease/minikube,type:*coordination.Lease,resource:leases.coordination.k8s.io 835ms (11:12:05.012)
Trace[1808226458]:  ---"Txn call completed" 835ms (11:12:05.848)]
Trace[1808226458]: [835.969256ms] [835.969256ms] END
I0721 11:15:03.723072       1 alloc.go:330] "allocated clusterIPs" service="default/nginx-services" clusterIPs={"IPv4":"10.102.86.13"}
I0721 11:15:12.778008       1 trace.go:236] Trace[103285177]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:acd63580-622e-470a-a6e4-31ef8a518b46,client:192.168.49.2,api-group:,api-version:v1,name:nginx-rs-match-expression-rwsxq,subresource:status,namespace:default,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/default/pods/nginx-rs-match-expression-rwsxq/status,user-agent:kubelet/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:PATCH (21-Jul-2024 11:15:12.221) (total time: 554ms):
Trace[103285177]: ["GuaranteedUpdate etcd3" audit-id:acd63580-622e-470a-a6e4-31ef8a518b46,key:/pods/default/nginx-rs-match-expression-rwsxq,type:*core.Pod,resource:pods 554ms (11:15:12.221)
Trace[103285177]:  ---"Txn call completed" 553ms (11:15:12.775)]
Trace[103285177]: ---"Object stored in database" 553ms (11:15:12.775)
Trace[103285177]: [554.714278ms] [554.714278ms] END
I0721 11:15:13.318531       1 trace.go:236] Trace[1998588446]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:8de91096-3f1a-4475-b84f-5c5a26045e93,client:192.168.49.2,api-group:apps,api-version:v1,name:nginx-rs-match-expression,subresource:status,namespace:default,protocol:HTTP/2.0,resource:replicasets,scope:resource,url:/apis/apps/v1/namespaces/default/replicasets/nginx-rs-match-expression/status,user-agent:kube-controller-manager/v1.30.0 (linux/amd64) kubernetes/7c48c2b/system:serviceaccount:kube-system:replicaset-controller,verb:PUT (21-Jul-2024 11:15:12.779) (total time: 538ms):
Trace[1998588446]: ["GuaranteedUpdate etcd3" audit-id:8de91096-3f1a-4475-b84f-5c5a26045e93,key:/replicasets/default/nginx-rs-match-expression,type:*apps.ReplicaSet,resource:replicasets.apps 538ms (11:15:12.779)
Trace[1998588446]:  ---"Txn call completed" 537ms (11:15:13.318)]
Trace[1998588446]: [538.729178ms] [538.729178ms] END
I0721 11:15:13.386704       1 trace.go:236] Trace[1768191782]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:0e1b171c-dbf6-4165-9cac-b4261cc39ba9,client:192.168.49.2,api-group:discovery.k8s.io,api-version:v1,name:nginx-services-4mtxg,subresource:,namespace:default,protocol:HTTP/2.0,resource:endpointslices,scope:resource,url:/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/nginx-services-4mtxg,user-agent:kube-controller-manager/v1.30.0 (linux/amd64) kubernetes/7c48c2b/system:serviceaccount:kube-system:endpointslice-controller,verb:PUT (21-Jul-2024 11:15:12.780) (total time: 605ms):
Trace[1768191782]: ["GuaranteedUpdate etcd3" audit-id:0e1b171c-dbf6-4165-9cac-b4261cc39ba9,key:/endpointslices/default/nginx-services-4mtxg,type:*discovery.EndpointSlice,resource:endpointslices.discovery.k8s.io 605ms (11:15:12.780)
Trace[1768191782]:  ---"Txn call completed" 605ms (11:15:13.386)]
Trace[1768191782]: [605.995222ms] [605.995222ms] END
I0721 11:15:13.386724       1 trace.go:236] Trace[1538398601]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:7082f340-8428-458e-a322-515b95afdbf8,client:192.168.49.2,api-group:,api-version:v1,name:nginx-services,subresource:,namespace:default,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/default/endpoints/nginx-services,user-agent:kube-controller-manager/v1.30.0 (linux/amd64) kubernetes/7c48c2b/system:serviceaccount:kube-system:endpoint-controller,verb:PUT (21-Jul-2024 11:15:12.780) (total time: 605ms):
Trace[1538398601]: ["GuaranteedUpdate etcd3" audit-id:7082f340-8428-458e-a322-515b95afdbf8,key:/services/endpoints/default/nginx-services,type:*core.Endpoints,resource:endpoints 605ms (11:15:12.780)
Trace[1538398601]:  ---"Txn call completed" 605ms (11:15:13.386)]
Trace[1538398601]: [605.769612ms] [605.769612ms] END
E0721 11:18:05.206639       1 conn.go:339] Error on socket receive: read tcp 192.168.49.2:8443->192.168.49.1:53492: use of closed network connection
E0721 11:18:09.171874       1 conn.go:339] Error on socket receive: read tcp 192.168.49.2:8443->192.168.49.1:47136: use of closed network connection
I0721 11:18:31.053506       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0721 11:18:31.185164       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0721 11:19:54.084818       1 alloc.go:330] "allocated clusterIPs" service="default/nginx-services" clusterIPs={"IPv4":"10.101.55.226"}
E0721 11:27:13.386020       1 conn.go:339] Error on socket receive: read tcp 192.168.49.2:8443->192.168.49.1:49576: use of closed network connection
E0721 11:32:09.210054       1 conn.go:339] Error on socket receive: read tcp 192.168.49.2:8443->192.168.49.1:56418: use of closed network connection
I0721 11:42:51.065096       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0721 11:42:51.082263       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
E0721 11:47:19.966520       1 conn.go:339] Error on socket receive: read tcp 192.168.49.2:8443->192.168.49.1:33578: use of closed network connection
I0721 11:47:31.058617       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0721 11:47:31.064545       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0721 11:56:56.469509       1 alloc.go:330] "allocated clusterIPs" service="default/service-nginx" clusterIPs={"IPv4":"10.108.106.109"}
E0721 12:01:47.742702       1 conn.go:339] Error on socket receive: read tcp 192.168.49.2:8443->192.168.49.1:48944: use of closed network connection
I0721 12:10:24.623187       1 trace.go:236] Trace[424499608]: "Delete" accept:application/json,audit-id:3a245b59-1d49-4172-a894-c87a85fba3e4,client:192.168.49.1,api-group:,api-version:v1,name:kubernetes,subresource:,namespace:default,protocol:HTTP/2.0,resource:services,scope:resource,url:/api/v1/namespaces/default/services/kubernetes,user-agent:kubectl/v1.30.3 (linux/amd64) kubernetes/6fc0a69,verb:DELETE (21-Jul-2024 12:10:23.846) (total time: 774ms):
Trace[424499608]: ---"Object deleted from database" 774ms (12:10:24.621)
Trace[424499608]: [774.965345ms] [774.965345ms] END
I0721 12:10:25.226607       1 trace.go:236] Trace[260291215]: "Delete" accept:application/json,audit-id:e69c707d-a0d0-441a-8901-5b0e82d6cc4a,client:192.168.49.1,api-group:,api-version:v1,name:service-nginx,subresource:,namespace:default,protocol:HTTP/2.0,resource:services,scope:resource,url:/api/v1/namespaces/default/services/service-nginx,user-agent:kubectl/v1.30.3 (linux/amd64) kubernetes/6fc0a69,verb:DELETE (21-Jul-2024 12:10:24.627) (total time: 599ms):
Trace[260291215]: ---"Object deleted from database" 599ms (12:10:25.226)
Trace[260291215]: [599.542231ms] [599.542231ms] END
I0721 12:10:31.078798       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0721 12:10:31.084965       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0721 12:18:27.231267       1 alloc.go:330] "allocated clusterIPs" service="default/service-nginx" clusterIPs={"IPv4":"10.111.211.163"}
I0721 12:49:31.121593       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0721 12:49:31.128066       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]


==> kube-apiserver [f2cabc1cbfd1] <==
W0721 13:40:46.330193       1 genericapiserver.go:733] Skipping API admissionregistration.k8s.io/v1beta1 because it has no resources.
W0721 13:40:46.330198       1 genericapiserver.go:733] Skipping API admissionregistration.k8s.io/v1alpha1 because it has no resources.
I0721 13:40:46.330485       1 handler.go:286] Adding GroupVersion events.k8s.io v1 to ResourceManager
W0721 13:40:46.330511       1 genericapiserver.go:733] Skipping API events.k8s.io/v1beta1 because it has no resources.
I0721 13:40:46.343776       1 handler.go:286] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
W0721 13:40:46.343946       1 genericapiserver.go:733] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I0721 13:40:46.618955       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0721 13:40:46.618992       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0721 13:40:46.619154       1 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0721 13:40:46.619269       1 secure_serving.go:213] Serving securely on [::]:8443
I0721 13:40:46.619308       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0721 13:40:46.619469       1 system_namespaces_controller.go:67] Starting system namespaces controller
I0721 13:40:46.619581       1 controller.go:78] Starting OpenAPI AggregationController
I0721 13:40:46.619733       1 controller.go:116] Starting legacy_token_tracking_controller
I0721 13:40:46.619763       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0721 13:40:46.620330       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0721 13:40:46.620363       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0721 13:40:46.620384       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0721 13:40:46.620471       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0721 13:40:46.620492       1 apf_controller.go:374] Starting API Priority and Fairness config controller
I0721 13:40:46.620544       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0721 13:40:46.620575       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0721 13:40:46.620590       1 available_controller.go:423] Starting AvailableConditionController
I0721 13:40:46.620594       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0721 13:40:46.620612       1 aggregator.go:163] waiting for initial CRD sync...
I0721 13:40:46.619583       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0721 13:40:46.620640       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0721 13:40:46.620664       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0721 13:40:46.620704       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0721 13:40:46.620794       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0721 13:40:46.621503       1 customresource_discovery_controller.go:289] Starting DiscoveryController
I0721 13:40:46.621570       1 controller.go:139] Starting OpenAPI controller
I0721 13:40:46.621601       1 controller.go:87] Starting OpenAPI V3 controller
I0721 13:40:46.621613       1 naming_controller.go:291] Starting NamingConditionController
I0721 13:40:46.621626       1 establishing_controller.go:76] Starting EstablishingController
I0721 13:40:46.621644       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0721 13:40:46.621656       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0721 13:40:46.621663       1 crd_finalizer.go:266] Starting CRDFinalizer
I0721 13:40:46.694789       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0721 13:40:46.704586       1 policy_source.go:224] refreshing policies
I0721 13:40:46.707060       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0721 13:40:46.719869       1 shared_informer.go:320] Caches are synced for configmaps
I0721 13:40:46.721049       1 apf_controller.go:379] Running API Priority and Fairness config worker
I0721 13:40:46.721075       1 apf_controller.go:382] Running API Priority and Fairness periodic rebalancing process
I0721 13:40:46.721098       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0721 13:40:46.721259       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0721 13:40:46.721279       1 aggregator.go:165] initial CRD sync complete...
I0721 13:40:46.721285       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0721 13:40:46.721290       1 autoregister_controller.go:141] Starting autoregister controller
I0721 13:40:46.721294       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0721 13:40:46.721298       1 cache.go:39] Caches are synced for autoregister controller
I0721 13:40:46.721297       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0721 13:40:46.725150       1 handler_discovery.go:447] Starting ResourceDiscoveryManager
I0721 13:40:46.748377       1 shared_informer.go:320] Caches are synced for node_authorizer
E0721 13:40:47.103493       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I0721 13:40:47.647794       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0721 13:40:47.692837       1 controller.go:615] quota admission added evaluator for: endpoints
I0721 13:40:59.775498       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0721 13:43:29.696806       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0721 13:43:29.779179       1 alloc.go:330] "allocated clusterIPs" service="default/service-nginx-loadbalancer" clusterIPs={"IPv4":"10.100.209.72"}


==> kube-controller-manager [2699cfb8219c] <==
I0721 11:18:22.152291       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="73.004¬µs"
I0721 11:18:22.157485       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="30.702¬µs"
I0721 11:18:22.164744       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="32.102¬µs"
I0721 11:18:22.182728       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="5.401¬µs"
I0721 11:19:54.054455       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="26.969881ms"
I0721 11:19:54.062455       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="7.962038ms"
I0721 11:19:54.062559       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="74.405¬µs"
I0721 11:19:54.070270       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="29.302¬µs"
I0721 11:19:54.076284       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="42.502¬µs"
I0721 11:19:57.977751       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="6.19544ms"
I0721 11:19:57.978249       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="444.524¬µs"
I0721 11:20:01.011705       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="6.643867ms"
I0721 11:20:01.011788       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="31.102¬µs"
I0721 11:20:03.029765       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="5.896226ms"
I0721 11:20:03.030415       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="39.902¬µs"
I0721 11:42:47.005382       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="29.880892ms"
I0721 11:42:47.085639       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="80.190637ms"
I0721 11:42:47.097576       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="11.899055ms"
I0721 11:42:47.103678       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="6.069683ms"
I0721 11:42:47.103774       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="43.202¬µs"
I0721 11:42:47.111947       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="39.901¬µs"
I0721 11:42:47.176394       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="7.1¬µs"
I0721 11:46:51.427652       1 cleaner.go:175] "Cleaning CSR as it is more than approvedExpiration duration old and approved." logger="certificatesigningrequest-cleaner-controller" csr="csr-m2qtg" approvedExpiration="1h0m0s"
I0721 11:56:56.438677       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="22.680885ms"
I0721 11:56:56.447887       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="9.170236ms"
I0721 11:56:56.456518       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="8.588122ms"
I0721 11:56:56.456589       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="39.701¬µs"
I0721 11:56:56.456807       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="15.9¬µs"
I0721 11:56:56.462688       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="35.001¬µs"
I0721 11:57:00.772362       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="4.857493ms"
I0721 11:57:00.772434       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="33.702¬µs"
I0721 11:57:02.795519       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="5.653941ms"
I0721 11:57:02.795576       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="36.102¬µs"
I0721 11:57:05.816575       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="5.381025ms"
I0721 11:57:05.816640       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="31.402¬µs"
I0721 12:10:23.640788       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="403.722914ms"
I0721 12:10:24.248342       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="607.51605ms"
I0721 12:10:24.455984       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="207.541592ms"
I0721 12:10:24.456037       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="34.401¬µs"
I0721 12:10:24.843370       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="36.701¬µs"
I0721 12:10:25.184090       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="34.1¬µs"
I0721 12:10:25.439023       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="33.901¬µs"
I0721 12:10:25.559820       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="36.3¬µs"
I0721 12:18:27.189913       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="17.350471ms"
I0721 12:18:27.206849       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="16.87535ms"
I0721 12:18:27.216198       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="9.314714ms"
I0721 12:18:27.216303       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="51.402¬µs"
I0721 12:18:27.223268       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="31.702¬µs"
I0721 12:18:27.235539       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="42.102¬µs"
I0721 12:18:31.001302       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="4.317993ms"
I0721 12:18:31.001400       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="37.201¬µs"
I0721 12:18:33.024042       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="7.872851ms"
I0721 12:18:33.024143       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="43.302¬µs"
I0721 12:18:37.129525       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="58.435458ms"
I0721 12:18:37.130083       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="110.505¬µs"
I0721 12:49:24.793690       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="42.504706ms"
I0721 12:49:24.841880       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="48.1236ms"
I0721 12:49:24.910008       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="68.086931ms"
I0721 12:49:24.910087       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="35.2¬µs"
I0721 12:49:24.934410       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="7¬µs"


==> kube-controller-manager [68fa5ec96872] <==
I0721 13:40:59.709598       1 node_lifecycle_controller.go:1227] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0721 13:40:59.709650       1 node_lifecycle_controller.go:879] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0721 13:40:59.709684       1 node_lifecycle_controller.go:1073] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0721 13:40:59.709790       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I0721 13:40:59.710899       1 shared_informer.go:320] Caches are synced for PV protection
I0721 13:40:59.738406       1 shared_informer.go:320] Caches are synced for job
I0721 13:40:59.740867       1 shared_informer.go:320] Caches are synced for service account
I0721 13:40:59.742070       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I0721 13:40:59.742103       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I0721 13:40:59.743257       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I0721 13:40:59.743286       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0721 13:40:59.750522       1 shared_informer.go:320] Caches are synced for TTL after finished
I0721 13:40:59.750561       1 shared_informer.go:320] Caches are synced for ephemeral
I0721 13:40:59.752878       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0721 13:40:59.759563       1 shared_informer.go:320] Caches are synced for expand
I0721 13:40:59.759993       1 shared_informer.go:320] Caches are synced for cronjob
I0721 13:40:59.760037       1 shared_informer.go:320] Caches are synced for TTL
I0721 13:40:59.760047       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I0721 13:40:59.761935       1 shared_informer.go:320] Caches are synced for endpoint
I0721 13:40:59.769220       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0721 13:40:59.771032       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I0721 13:40:59.772236       1 shared_informer.go:320] Caches are synced for node
I0721 13:40:59.772299       1 range_allocator.go:175] "Sending events to api server" logger="node-ipam-controller"
I0721 13:40:59.772313       1 range_allocator.go:179] "Starting range CIDR allocator" logger="node-ipam-controller"
I0721 13:40:59.772320       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0721 13:40:59.772324       1 shared_informer.go:320] Caches are synced for cidrallocator
I0721 13:40:59.774051       1 shared_informer.go:320] Caches are synced for ReplicationController
I0721 13:40:59.775264       1 shared_informer.go:320] Caches are synced for GC
I0721 13:40:59.777597       1 shared_informer.go:320] Caches are synced for daemon sets
I0721 13:40:59.779959       1 shared_informer.go:320] Caches are synced for attach detach
I0721 13:40:59.781120       1 shared_informer.go:320] Caches are synced for PVC protection
I0721 13:40:59.783411       1 shared_informer.go:320] Caches are synced for crt configmap
I0721 13:40:59.785921       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0721 13:40:59.793317       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0721 13:40:59.834833       1 shared_informer.go:320] Caches are synced for HPA
I0721 13:40:59.903587       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0721 13:40:59.919986       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="16.335925ms"
I0721 13:40:59.920595       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="30.997¬µs"
I0721 13:40:59.957385       1 shared_informer.go:320] Caches are synced for deployment
I0721 13:40:59.971642       1 shared_informer.go:320] Caches are synced for stateful set
I0721 13:40:59.987599       1 shared_informer.go:320] Caches are synced for resource quota
I0721 13:40:59.993388       1 shared_informer.go:320] Caches are synced for resource quota
I0721 13:41:00.007317       1 shared_informer.go:320] Caches are synced for disruption
I0721 13:41:00.404351       1 shared_informer.go:320] Caches are synced for garbage collector
I0721 13:41:00.428707       1 shared_informer.go:320] Caches are synced for garbage collector
I0721 13:41:00.428749       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I0721 13:41:01.489827       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="7.018336ms"
I0721 13:41:01.489956       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="53.106¬µs"
I0721 13:43:29.734110       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="31.405484ms"
I0721 13:43:29.743269       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="9.102933ms"
I0721 13:43:29.743492       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="65.004¬µs"
I0721 13:43:29.748646       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="29.402¬µs"
I0721 13:43:29.757039       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="35.802¬µs"
I0721 13:43:29.764648       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="64.104¬µs"
I0721 13:43:35.459126       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="3.861932ms"
I0721 13:43:35.459205       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="36.702¬µs"
I0721 13:43:39.497812       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="5.123475ms"
I0721 13:43:39.497896       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="32.801¬µs"
I0721 13:43:43.535511       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="6.414279ms"
I0721 13:43:43.535613       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-rs-match-expression" duration="57.002¬µs"


==> kube-proxy [44100b19f879] <==
I0721 13:40:54.330594       1 server_linux.go:69] "Using iptables proxy"
I0721 13:40:54.393491       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
I0721 13:40:54.488947       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0721 13:40:54.489009       1 server_linux.go:165] "Using iptables Proxier"
I0721 13:40:54.493170       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0721 13:40:54.493211       1 server_linux.go:528] "Defaulting to no-op detect-local"
I0721 13:40:54.493244       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0721 13:40:54.493609       1 server.go:872] "Version info" version="v1.30.0"
I0721 13:40:54.493712       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0721 13:40:54.497156       1 config.go:192] "Starting service config controller"
I0721 13:40:54.497215       1 shared_informer.go:313] Waiting for caches to sync for service config
I0721 13:40:54.497524       1 config.go:101] "Starting endpoint slice config controller"
I0721 13:40:54.497698       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0721 13:40:54.497547       1 config.go:319] "Starting node config controller"
I0721 13:40:54.497743       1 shared_informer.go:313] Waiting for caches to sync for node config
I0721 13:40:54.597306       1 shared_informer.go:320] Caches are synced for service config
I0721 13:40:54.598489       1 shared_informer.go:320] Caches are synced for node config
I0721 13:40:54.598528       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-proxy [505681e92107] <==
I0721 09:47:06.433149       1 server_linux.go:69] "Using iptables proxy"
I0721 09:47:06.439840       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
I0721 09:47:06.465067       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0721 09:47:06.465097       1 server_linux.go:165] "Using iptables Proxier"
I0721 09:47:06.466717       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0721 09:47:06.466748       1 server_linux.go:528] "Defaulting to no-op detect-local"
I0721 09:47:06.466765       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0721 09:47:06.466953       1 server.go:872] "Version info" version="v1.30.0"
I0721 09:47:06.467264       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0721 09:47:06.468168       1 config.go:192] "Starting service config controller"
I0721 09:47:06.468182       1 shared_informer.go:313] Waiting for caches to sync for service config
I0721 09:47:06.468220       1 config.go:101] "Starting endpoint slice config controller"
I0721 09:47:06.468225       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0721 09:47:06.469623       1 config.go:319] "Starting node config controller"
I0721 09:47:06.469633       1 shared_informer.go:313] Waiting for caches to sync for node config
I0721 09:47:06.569327       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0721 09:47:06.569408       1 shared_informer.go:320] Caches are synced for service config
I0721 09:47:06.569829       1 shared_informer.go:320] Caches are synced for node config


==> kube-scheduler [6e298e10f028] <==
I0721 13:40:44.906890       1 serving.go:380] Generated self-signed cert in-memory
W0721 13:40:46.694563       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0721 13:40:46.694646       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system": RBAC: [role.rbac.authorization.k8s.io "system::leader-locking-kube-scheduler" not found, role.rbac.authorization.k8s.io "extension-apiserver-authentication-reader" not found]
W0721 13:40:46.694677       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0721 13:40:46.694723       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0721 13:40:46.840321       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.30.0"
I0721 13:40:46.840353       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0721 13:40:46.868522       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0721 13:40:46.895571       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0721 13:40:46.895579       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0721 13:40:46.895702       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0721 13:40:47.095886       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kube-scheduler [ffd688061026] <==
E0721 09:46:45.622793       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0721 09:46:45.622473       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0721 09:46:45.622802       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0721 09:46:45.622499       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0721 09:46:45.622812       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0721 09:46:45.622525       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0721 09:46:45.622821       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0721 09:46:45.622576       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0721 09:46:45.622833       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0721 09:46:45.623590       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0721 09:46:45.623613       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0721 09:46:46.459347       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0721 09:46:46.459374       1 reflector.go:547] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0721 09:46:46.459394       1 reflector.go:150] runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0721 09:46:46.459394       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0721 09:46:46.533284       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0721 09:46:46.533333       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0721 09:46:46.612408       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0721 09:46:46.612451       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0721 09:46:46.616714       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0721 09:46:46.616762       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0721 09:46:46.653429       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0721 09:46:46.653471       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0721 09:46:46.666796       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0721 09:46:46.666847       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0721 09:46:46.687912       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0721 09:46:46.687986       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0721 09:46:46.814502       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0721 09:46:46.814586       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0721 09:46:46.961705       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0721 09:46:46.961803       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0721 09:46:47.048173       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0721 09:46:47.048215       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0721 09:46:47.137606       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0721 09:46:47.137649       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0721 09:46:47.138320       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0721 09:46:47.138351       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0721 09:46:47.140390       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0721 09:46:47.140425       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0721 09:46:47.198516       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0721 09:46:47.198556       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0721 09:46:48.252214       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0721 09:46:48.252254       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0721 09:46:48.380638       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0721 09:46:48.380681       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0721 09:46:48.502351       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0721 09:46:48.502413       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0721 09:46:48.682772       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0721 09:46:48.682814       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0721 09:46:48.795918       1 reflector.go:547] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0721 09:46:48.795959       1 reflector.go:150] runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0721 09:46:49.325941       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0721 09:46:49.325985       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0721 09:46:49.332130       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0721 09:46:49.332169       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0721 09:46:49.335280       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0721 09:46:49.335312       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0721 09:46:49.353067       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0721 09:46:49.353113       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
I0721 09:46:55.120063       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Jul 21 13:40:43 minikube kubelet[1433]: I0721 13:40:43.000891    1433 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/3c555f828409b009ebee39fdbedfcac0-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"3c555f828409b009ebee39fdbedfcac0\") " pod="kube-system/kube-apiserver-minikube"
Jul 21 13:40:43 minikube kubelet[1433]: I0721 13:40:43.000902    1433 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/3c555f828409b009ebee39fdbedfcac0-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"3c555f828409b009ebee39fdbedfcac0\") " pod="kube-system/kube-apiserver-minikube"
Jul 21 13:40:43 minikube kubelet[1433]: I0721 13:40:43.061321    1433 kubelet_node_status.go:73] "Attempting to register node" node="minikube"
Jul 21 13:40:43 minikube kubelet[1433]: E0721 13:40:43.061616    1433 kubelet_node_status.go:96] "Unable to register node with API server" err="Post \"https://control-plane.minikube.internal:8443/api/v1/nodes\": dial tcp 192.168.49.2:8443: connect: connection refused" node="minikube"
Jul 21 13:40:43 minikube kubelet[1433]: E0721 13:40:43.365084    1433 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused" interval="800ms"
Jul 21 13:40:43 minikube kubelet[1433]: I0721 13:40:43.492876    1433 kubelet_node_status.go:73] "Attempting to register node" node="minikube"
Jul 21 13:40:43 minikube kubelet[1433]: E0721 13:40:43.493164    1433 kubelet_node_status.go:96] "Unable to register node with API server" err="Post \"https://control-plane.minikube.internal:8443/api/v1/nodes\": dial tcp 192.168.49.2:8443: connect: connection refused" node="minikube"
Jul 21 13:40:44 minikube kubelet[1433]: W0721 13:40:44.000687    1433 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get "https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Jul 21 13:40:44 minikube kubelet[1433]: E0721 13:40:44.000750    1433 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Jul 21 13:40:44 minikube kubelet[1433]: W0721 13:40:44.111448    1433 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "https://control-plane.minikube.internal:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Jul 21 13:40:44 minikube kubelet[1433]: E0721 13:40:44.111511    1433 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://control-plane.minikube.internal:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Jul 21 13:40:44 minikube kubelet[1433]: E0721 13:40:44.166279    1433 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused" interval="1.6s"
Jul 21 13:40:44 minikube kubelet[1433]: W0721 13:40:44.272415    1433 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get "https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%!D(MISSING)minikube&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Jul 21 13:40:44 minikube kubelet[1433]: E0721 13:40:44.272486    1433 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%!D(MISSING)minikube&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Jul 21 13:40:44 minikube kubelet[1433]: I0721 13:40:44.294674    1433 kubelet_node_status.go:73] "Attempting to register node" node="minikube"
Jul 21 13:40:44 minikube kubelet[1433]: E0721 13:40:44.295022    1433 kubelet_node_status.go:96] "Unable to register node with API server" err="Post \"https://control-plane.minikube.internal:8443/api/v1/nodes\": dial tcp 192.168.49.2:8443: connect: connection refused" node="minikube"
Jul 21 13:40:44 minikube kubelet[1433]: W0721 13:40:44.319183    1433 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.RuntimeClass: Get "https://control-plane.minikube.internal:8443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Jul 21 13:40:44 minikube kubelet[1433]: E0721 13:40:44.319250    1433 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: Get "https://control-plane.minikube.internal:8443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Jul 21 13:40:45 minikube kubelet[1433]: I0721 13:40:45.897320    1433 kubelet_node_status.go:73] "Attempting to register node" node="minikube"
Jul 21 13:40:46 minikube kubelet[1433]: I0721 13:40:46.739101    1433 apiserver.go:52] "Watching apiserver"
Jul 21 13:40:46 minikube kubelet[1433]: I0721 13:40:46.841243    1433 topology_manager.go:215] "Topology Admit Handler" podUID="484b00d1-2780-4c58-988e-209f09c5789e" podNamespace="kube-system" podName="storage-provisioner"
Jul 21 13:40:46 minikube kubelet[1433]: I0721 13:40:46.842057    1433 topology_manager.go:215] "Topology Admit Handler" podUID="c4846a43-b67d-4c5f-b780-8a13472cc79a" podNamespace="kube-system" podName="coredns-7db6d8ff4d-jwqcj"
Jul 21 13:40:46 minikube kubelet[1433]: I0721 13:40:46.842144    1433 topology_manager.go:215] "Topology Admit Handler" podUID="8f1ec671-1fef-4900-abf4-e2a3022388aa" podNamespace="kube-system" podName="kube-proxy-xt2m7"
Jul 21 13:40:46 minikube kubelet[1433]: I0721 13:40:46.859884    1433 desired_state_of_world_populator.go:157] "Finished populating initial desired state of world"
Jul 21 13:40:46 minikube kubelet[1433]: I0721 13:40:46.904135    1433 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/484b00d1-2780-4c58-988e-209f09c5789e-tmp\") pod \"storage-provisioner\" (UID: \"484b00d1-2780-4c58-988e-209f09c5789e\") " pod="kube-system/storage-provisioner"
Jul 21 13:40:46 minikube kubelet[1433]: I0721 13:40:46.904225    1433 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/8f1ec671-1fef-4900-abf4-e2a3022388aa-xtables-lock\") pod \"kube-proxy-xt2m7\" (UID: \"8f1ec671-1fef-4900-abf4-e2a3022388aa\") " pod="kube-system/kube-proxy-xt2m7"
Jul 21 13:40:46 minikube kubelet[1433]: I0721 13:40:46.904249    1433 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/8f1ec671-1fef-4900-abf4-e2a3022388aa-lib-modules\") pod \"kube-proxy-xt2m7\" (UID: \"8f1ec671-1fef-4900-abf4-e2a3022388aa\") " pod="kube-system/kube-proxy-xt2m7"
Jul 21 13:40:47 minikube kubelet[1433]: I0721 13:40:47.107217    1433 kubelet_node_status.go:112] "Node was previously registered" node="minikube"
Jul 21 13:40:47 minikube kubelet[1433]: I0721 13:40:47.107302    1433 kubelet_node_status.go:76] "Successfully registered node" node="minikube"
Jul 21 13:40:47 minikube kubelet[1433]: I0721 13:40:47.108885    1433 kuberuntime_manager.go:1523] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Jul 21 13:40:47 minikube kubelet[1433]: I0721 13:40:47.109541    1433 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Jul 21 13:40:47 minikube kubelet[1433]: E0721 13:40:47.165349    1433 kubelet.go:1928] "Failed creating a mirror pod for" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Jul 21 13:40:49 minikube kubelet[1433]: I0721 13:40:49.938106    1433 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="5eb24b1d1107c1d9e0f06598fba89a88fdc2fc598e00017acaa23c641ca5ee6d"
Jul 21 13:40:49 minikube kubelet[1433]: I0721 13:40:49.951417    1433 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="b0e41b4f490674c41ae6092f9b8071a26e1a83efc55ac6ec99b18e9bed5acd78"
Jul 21 13:40:52 minikube kubelet[1433]: E0721 13:40:52.906052    1433 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Jul 21 13:40:52 minikube kubelet[1433]: E0721 13:40:52.906095    1433 helpers.go:857] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
Jul 21 13:40:54 minikube kubelet[1433]: I0721 13:40:54.367983    1433 scope.go:117] "RemoveContainer" containerID="6c615f22956371a76f62600827c6cf88a0f6fb4a1f3b7ed97c477aa3ece24a30"
Jul 21 13:40:54 minikube kubelet[1433]: I0721 13:40:54.368166    1433 scope.go:117] "RemoveContainer" containerID="88da2a63a28198b4bdea55be0e1932a961b3bb931b54412bdf500919a90f258b"
Jul 21 13:40:54 minikube kubelet[1433]: E0721 13:40:54.368374    1433 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(484b00d1-2780-4c58-988e-209f09c5789e)\"" pod="kube-system/storage-provisioner" podUID="484b00d1-2780-4c58-988e-209f09c5789e"
Jul 21 13:41:02 minikube kubelet[1433]: E0721 13:41:02.927942    1433 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Jul 21 13:41:02 minikube kubelet[1433]: E0721 13:41:02.927981    1433 helpers.go:857] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
Jul 21 13:41:05 minikube kubelet[1433]: I0721 13:41:05.799265    1433 scope.go:117] "RemoveContainer" containerID="88da2a63a28198b4bdea55be0e1932a961b3bb931b54412bdf500919a90f258b"
Jul 21 13:41:12 minikube kubelet[1433]: E0721 13:41:12.948896    1433 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Jul 21 13:41:12 minikube kubelet[1433]: E0721 13:41:12.948944    1433 helpers.go:857] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
Jul 21 13:41:22 minikube kubelet[1433]: E0721 13:41:22.968440    1433 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Jul 21 13:41:22 minikube kubelet[1433]: E0721 13:41:22.968486    1433 helpers.go:857] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
Jul 21 13:41:32 minikube kubelet[1433]: E0721 13:41:32.981325    1433 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Jul 21 13:41:32 minikube kubelet[1433]: E0721 13:41:32.981367    1433 helpers.go:857] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
Jul 21 13:43:29 minikube kubelet[1433]: I0721 13:43:29.725760    1433 topology_manager.go:215] "Topology Admit Handler" podUID="2d564609-28f6-4c07-b313-9fc43682e51e" podNamespace="default" podName="nginx-rs-match-expression-htrzc"
Jul 21 13:43:29 minikube kubelet[1433]: I0721 13:43:29.737149    1433 topology_manager.go:215] "Topology Admit Handler" podUID="72e76fb7-314e-4bfc-9c27-e327c764a4f2" podNamespace="default" podName="nginx-rs-match-expression-9f7fr"
Jul 21 13:43:29 minikube kubelet[1433]: I0721 13:43:29.737289    1433 topology_manager.go:215] "Topology Admit Handler" podUID="968148c1-33bd-4c2a-95b7-c187aa3b2559" podNamespace="default" podName="nginx-rs-match-expression-nlvcz"
Jul 21 13:43:29 minikube kubelet[1433]: I0721 13:43:29.840055    1433 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-gwpfr\" (UniqueName: \"kubernetes.io/projected/72e76fb7-314e-4bfc-9c27-e327c764a4f2-kube-api-access-gwpfr\") pod \"nginx-rs-match-expression-9f7fr\" (UID: \"72e76fb7-314e-4bfc-9c27-e327c764a4f2\") " pod="default/nginx-rs-match-expression-9f7fr"
Jul 21 13:43:29 minikube kubelet[1433]: I0721 13:43:29.840109    1433 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-nfpq7\" (UniqueName: \"kubernetes.io/projected/968148c1-33bd-4c2a-95b7-c187aa3b2559-kube-api-access-nfpq7\") pod \"nginx-rs-match-expression-nlvcz\" (UID: \"968148c1-33bd-4c2a-95b7-c187aa3b2559\") " pod="default/nginx-rs-match-expression-nlvcz"
Jul 21 13:43:29 minikube kubelet[1433]: I0721 13:43:29.840127    1433 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-r8ck6\" (UniqueName: \"kubernetes.io/projected/2d564609-28f6-4c07-b313-9fc43682e51e-kube-api-access-r8ck6\") pod \"nginx-rs-match-expression-htrzc\" (UID: \"2d564609-28f6-4c07-b313-9fc43682e51e\") " pod="default/nginx-rs-match-expression-htrzc"
Jul 21 13:43:30 minikube kubelet[1433]: I0721 13:43:30.303333    1433 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="48094a285bee280f7aebe9403274d78155803b58f7b00cea8fe24f16bf1b1118"
Jul 21 13:43:30 minikube kubelet[1433]: I0721 13:43:30.345490    1433 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="95b4cc61cfa4544f9593ddb542a54820fb71b25c21514ec98cf68b60d9aced06"
Jul 21 13:43:30 minikube kubelet[1433]: I0721 13:43:30.385101    1433 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="60f3e84fed6c1f73a030a6d965d0cab7f5b04967bbb26833f3f60b8c80c5d3b1"
Jul 21 13:43:39 minikube kubelet[1433]: I0721 13:43:39.492748    1433 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/nginx-rs-match-expression-htrzc" podStartSLOduration=6.502598689 podStartE2EDuration="10.49273288s" podCreationTimestamp="2024-07-21 13:43:29 +0000 UTC" firstStartedPulling="2024-07-21 13:43:30.396071216 +0000 UTC m=+168.092274314" lastFinishedPulling="2024-07-21 13:43:34.386205307 +0000 UTC m=+172.082408505" observedRunningTime="2024-07-21 13:43:35.45569001 +0000 UTC m=+173.151893208" watchObservedRunningTime="2024-07-21 13:43:39.49273288 +0000 UTC m=+177.188935978"
Jul 21 13:43:43 minikube kubelet[1433]: I0721 13:43:43.529056    1433 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/nginx-rs-match-expression-nlvcz" podStartSLOduration=1.609292173 podStartE2EDuration="14.529043108s" podCreationTimestamp="2024-07-21 13:43:29 +0000 UTC" firstStartedPulling="2024-07-21 13:43:30.429270225 +0000 UTC m=+168.125473323" lastFinishedPulling="2024-07-21 13:43:43.34902116 +0000 UTC m=+181.045224258" observedRunningTime="2024-07-21 13:43:43.528983105 +0000 UTC m=+181.225186203" watchObservedRunningTime="2024-07-21 13:43:43.529043108 +0000 UTC m=+181.225246206"
Jul 21 13:43:43 minikube kubelet[1433]: I0721 13:43:43.529173    1433 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/nginx-rs-match-expression-9f7fr" podStartSLOduration=6.494753066 podStartE2EDuration="14.529168213s" podCreationTimestamp="2024-07-21 13:43:29 +0000 UTC" firstStartedPulling="2024-07-21 13:43:30.417602114 +0000 UTC m=+168.113805212" lastFinishedPulling="2024-07-21 13:43:38.452017261 +0000 UTC m=+176.148220359" observedRunningTime="2024-07-21 13:43:39.492834684 +0000 UTC m=+177.189037882" watchObservedRunningTime="2024-07-21 13:43:43.529168213 +0000 UTC m=+181.225371311"


==> storage-provisioner [88da2a63a281] <==
I0721 13:40:53.021717       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0721 13:40:53.379285       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": x509: certificate signed by unknown authority


==> storage-provisioner [f6ace3b4a0f5] <==
I0721 13:41:05.923949       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0721 13:41:05.933476       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0721 13:41:05.934105       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0721 13:41:23.334714       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0721 13:41:23.334816       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"5ae07bd3-9e33-4956-937d-b02290a42e78", APIVersion:"v1", ResourceVersion:"11507", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_02132645-cd1a-4d9f-a29c-56b9baeb5b1c became leader
I0721 13:41:23.334844       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_02132645-cd1a-4d9f-a29c-56b9baeb5b1c!
I0721 13:41:23.435812       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_02132645-cd1a-4d9f-a29c-56b9baeb5b1c!

